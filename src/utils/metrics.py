"""
è·¨åŸŸç½®ä¿¡åº¦è®¡ç®—å™¨ - UniMatch-Clipé¡¹ç›®æ ¸å¿ƒç»„ä»¶

ç»Ÿä¸€å››ä¸ªé¢†åŸŸçš„ç½®ä¿¡åº¦è®¡ç®—æ¥å£ï¼š
- Vision: ç›®æ ‡æ£€æµ‹/åˆ†ç±»ç½®ä¿¡åº¦ (max_prob + bbox_iou + spatial_consistency)
- NLP: æ–‡æœ¬åˆ†ç±»ç½®ä¿¡åº¦ (softmax_entropy + attention_consistency + linguistic_features)
- Security: å¼‚å¸¸æ£€æµ‹ç½®ä¿¡åº¦ (anomaly_score + ensemble_agreement + feature_stability)
- Medical: è¯Šæ–­ç½®ä¿¡åº¦ (diagnosis_prob + uncertainty_estimation + clinical_consistency)

ç›®æ ‡ï¼šå°†ä¸åŒé¢†åŸŸçš„æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸º [0,1] ç»Ÿä¸€ç½®ä¿¡åº¦åˆ†æ•°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Optional, Union, Tuple, Any
import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
import warnings


@dataclass
class ConfidenceOutput:
    """ç½®ä¿¡åº¦è®¡ç®—ç»“æœ"""
    confidence: float  # ä¸»ç½®ä¿¡åº¦åˆ†æ•° [0,1]
    components: Dict[str, float]  # å„ç»„ä»¶åˆ†æ•°
    metadata: Dict[str, Any]  # é¢å¤–ä¿¡æ¯
    domain: str  # æ‰€å±é¢†åŸŸ


class BaseConfidenceCalculator(ABC):
    """ç½®ä¿¡åº¦è®¡ç®—å™¨åŸºç±»"""

    def __init__(self, domain: str, normalize: bool = True):
        self.domain = domain
        self.normalize = normalize
        self.logger = logging.getLogger(f"Confidence_{domain}")

    @abstractmethod
    def compute_confidence(self, model_output: Dict[str, torch.Tensor],
                           inputs: Optional[torch.Tensor] = None,
                           **kwargs) -> ConfidenceOutput:
        """è®¡ç®—ç½®ä¿¡åº¦çš„æŠ½è±¡æ–¹æ³•"""
        pass

    def _normalize_score(self, score: float, min_val: float = 0.0,
                         max_val: float = 1.0) -> float:
        """å½’ä¸€åŒ–åˆ†æ•°åˆ°[0,1]"""
        if not self.normalize:
            return score
        return np.clip((score - min_val) / (max_val - min_val), 0.0, 1.0)


class VisionConfidenceCalculator(BaseConfidenceCalculator):
    """
    Visioné¢†åŸŸç½®ä¿¡åº¦è®¡ç®—å™¨

    æ”¯æŒåœºæ™¯ï¼š
    - å›¾åƒåˆ†ç±»: softmaxæ¦‚ç‡ + ç‰¹å¾ä¸€è‡´æ€§
    - ç›®æ ‡æ£€æµ‹: bboxç½®ä¿¡åº¦ + IoU + ç©ºé—´ä¸€è‡´æ€§
    - è¯­ä¹‰åˆ†å‰²: åƒç´ çº§ç½®ä¿¡åº¦ + åŒºåŸŸä¸€è‡´æ€§
    """

    def __init__(self, task_type='classification', iou_threshold=0.5):
        super().__init__('vision')
        self.task_type = task_type  # 'classification', 'detection', 'segmentation'
        self.iou_threshold = iou_threshold

    def compute_confidence(self, model_output: Dict[str, torch.Tensor],
                           inputs: Optional[torch.Tensor] = None,
                           **kwargs) -> ConfidenceOutput:
        """
        è®¡ç®—Visionç½®ä¿¡åº¦

        Args:
            model_output: æ¨¡å‹è¾“å‡ºå­—å…¸ï¼ŒåŒ…å«ï¼š
                - 'logits': [B, num_classes] åˆ†ç±»logits
                - 'boxes': [B, N, 4] bboxåæ ‡ (å¯é€‰)
                - 'scores': [B, N] bboxç½®ä¿¡åº¦åˆ†æ•° (å¯é€‰)
                - 'features': [B, D] ç‰¹å¾å‘é‡ (å¯é€‰)
            inputs: [B, C, H, W] è¾“å…¥å›¾åƒå¼ é‡ (å¯é€‰)

        Returns:
            ConfidenceOutputå¯¹è±¡
        """

        components = {}
        metadata = {}

        if self.task_type == 'classification':
            confidence = self._compute_classification_confidence(
                model_output, inputs, components, metadata
            )
        elif self.task_type == 'detection':
            confidence = self._compute_detection_confidence(
                model_output, inputs, components, metadata
            )
        else:
            # é»˜è®¤åˆ†ç±»æ–¹å¼
            confidence = self._compute_classification_confidence(
                model_output, inputs, components, metadata
            )

        return ConfidenceOutput(
            confidence=confidence,
            components=components,
            metadata=metadata,
            domain='vision'
        )

    def _compute_classification_confidence(self, model_output: Dict,
                                           inputs: Optional[torch.Tensor],
                                           components: Dict, metadata: Dict) -> float:
        """è®¡ç®—åˆ†ç±»ä»»åŠ¡ç½®ä¿¡åº¦"""

        logits = model_output.get('logits')
        if logits is None:
            raise ValueError("åˆ†ç±»ä»»åŠ¡éœ€è¦'logits'è¾“å‡º")

        # 1. æœ€å¤§æ¦‚ç‡åˆ†æ•° (ä¸»è¦æŒ‡æ ‡)
        probs = F.softmax(logits, dim=-1)
        max_probs = probs.max(dim=-1)[0]  # [B]
        max_prob_score = max_probs.mean().item()
        components['max_probability'] = max_prob_score

        # 2. ç†µåˆ†æ•° (ä¸ç¡®å®šæ€§åº¦é‡)
        entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=-1)  # [B]
        max_entropy = np.log(probs.size(-1))  # log(num_classes)
        entropy_score = 1.0 - (entropy.mean().item() / max_entropy)
        components['entropy_confidence'] = entropy_score

        # 3. é¢„æµ‹è¾¹é™… (top1ä¸top2å·®è·)
        sorted_probs = torch.sort(probs, dim=-1, descending=True)[0]
        if sorted_probs.size(-1) > 1:
            margin = sorted_probs[:, 0] - sorted_probs[:, 1]  # [B]
            margin_score = margin.mean().item()
        else:
            margin_score = max_prob_score
        components['prediction_margin'] = margin_score

        # 4. ç‰¹å¾ä¸€è‡´æ€§ (å¦‚æœæœ‰ç‰¹å¾)
        feature_consistency = 0.0
        if 'features' in model_output:
            features = model_output['features']  # [B, D]
            if features.size(0) > 1:
                # è®¡ç®—æ‰¹æ¬¡å†…ç‰¹å¾ç›¸ä¼¼åº¦
                normalized_features = F.normalize(features, dim=-1)
                similarity_matrix = torch.mm(normalized_features, normalized_features.t())
                # æ’é™¤å¯¹è§’çº¿ï¼Œè®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
                mask = torch.eye(features.size(0), device=features.device).bool()
                similarity_matrix = similarity_matrix.masked_fill(mask, 0.0)
                feature_consistency = similarity_matrix.mean().item()
        components['feature_consistency'] = feature_consistency

        # 5. ç»¼åˆç½®ä¿¡åº¦è®¡ç®— (åŠ æƒå¹³å‡)
        weights = {
            'max_probability': 0.4,
            'entropy_confidence': 0.3,
            'prediction_margin': 0.2,
            'feature_consistency': 0.1
        }

        final_confidence = sum(
            components[key] * weights[key] for key in weights.keys()
        )

        # è®°å½•å…ƒæ•°æ®
        metadata.update({
            'task_type': 'classification',
            'num_classes': probs.size(-1),
            'batch_size': probs.size(0),
            'weights_used': weights
        })

        return final_confidence

    def _compute_detection_confidence(self, model_output: Dict,
                                      inputs: Optional[torch.Tensor],
                                      components: Dict, metadata: Dict) -> float:
        """è®¡ç®—ç›®æ ‡æ£€æµ‹ç½®ä¿¡åº¦"""

        boxes = model_output.get('boxes')  # [B, N, 4]
        scores = model_output.get('scores')  # [B, N]

        if boxes is None or scores is None:
            raise ValueError("æ£€æµ‹ä»»åŠ¡éœ€è¦'boxes'å’Œ'scores'è¾“å‡º")

        # 1. æ£€æµ‹åˆ†æ•° (ä¸»è¦æŒ‡æ ‡)
        detection_scores = scores.mean().item()
        components['detection_scores'] = detection_scores

        # 2. bboxè´¨é‡è¯„ä¼° (åŸºäºé¢ç§¯å’Œä½ç½®)
        box_areas = self._compute_box_areas(boxes)  # [B, N]
        # è¿‡æ»¤æ‰é¢ç§¯è¿‡å°çš„bbox
        valid_boxes = box_areas > 0.01  # è‡³å°‘å å›¾åƒ1%
        if valid_boxes.sum() > 0:
            box_quality = (box_areas * valid_boxes.float()).sum() / valid_boxes.sum()
            box_quality = box_quality.item()
        else:
            box_quality = 0.0
        components['box_quality'] = box_quality

        # 3. ç©ºé—´ä¸€è‡´æ€§ (æ£€æµ‹æ¡†åˆ†å¸ƒ)
        spatial_consistency = self._compute_spatial_consistency(boxes)
        components['spatial_consistency'] = spatial_consistency

        # 4. NMSåçš„ä¿ç•™ç‡ (é—´æ¥è´¨é‡æŒ‡æ ‡)
        nms_retention = self._estimate_nms_retention(boxes, scores)
        components['nms_retention'] = nms_retention

        # 5. ç»¼åˆç½®ä¿¡åº¦
        weights = {
            'detection_scores': 0.5,
            'box_quality': 0.2,
            'spatial_consistency': 0.15,
            'nms_retention': 0.15
        }

        final_confidence = sum(
            components[key] * weights[key] for key in weights.keys()
        )

        metadata.update({
            'task_type': 'detection',
            'num_detections': boxes.size(1),
            'batch_size': boxes.size(0),
            'iou_threshold': self.iou_threshold
        })

        return final_confidence

    def _compute_box_areas(self, boxes: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—bboxé¢ç§¯ [B, N]"""
        # boxes: [B, N, 4] (x1, y1, x2, y2)
        widths = boxes[:, :, 2] - boxes[:, :, 0]
        heights = boxes[:, :, 3] - boxes[:, :, 1]
        areas = widths * heights
        return torch.clamp(areas, min=0.0)

    def _compute_spatial_consistency(self, boxes: torch.Tensor) -> float:
        """è®¡ç®—æ£€æµ‹æ¡†ç©ºé—´åˆ†å¸ƒä¸€è‡´æ€§"""
        # ç®€åŒ–å®ç°ï¼šè®¡ç®—bboxä¸­å¿ƒç‚¹çš„åˆ†å¸ƒå‡åŒ€æ€§
        centers = boxes[:, :, :2] + (boxes[:, :, 2:] - boxes[:, :, :2]) / 2
        # è®¡ç®—ä¸­å¿ƒç‚¹çš„æ ‡å‡†å·®ï¼Œä½œä¸ºåˆ†å¸ƒä¸€è‡´æ€§çš„é€†æŒ‡æ ‡
        std_x = centers[:, :, 0].std().item()
        std_y = centers[:, :, 1].std().item()
        consistency = 1.0 / (1.0 + std_x + std_y)
        return consistency

    def _estimate_nms_retention(self, boxes: torch.Tensor,
                                scores: torch.Tensor) -> float:
        """ä¼°ç®—NMSåçš„ä¿ç•™ç‡"""
        # ç®€åŒ–å®ç°ï¼šé«˜åˆ†æ£€æµ‹æ¡†çš„å æ¯”
        high_score_mask = scores > 0.5
        retention_rate = high_score_mask.float().mean().item()
        return retention_rate


class NLPConfidenceCalculator(BaseConfidenceCalculator):
    """
    NLPé¢†åŸŸç½®ä¿¡åº¦è®¡ç®—å™¨

    æ”¯æŒåœºæ™¯ï¼š
    - æ–‡æœ¬åˆ†ç±»: softmaxç½®ä¿¡åº¦ + attentionä¸€è‡´æ€§
    - åºåˆ—æ ‡æ³¨: tokençº§ç½®ä¿¡åº¦èšåˆ
    - ç”Ÿæˆä»»åŠ¡: ç”Ÿæˆè´¨é‡ç½®ä¿¡åº¦
    """

    def __init__(self, task_type='classification', attention_layers=None):
        super().__init__('nlp')
        self.task_type = task_type
        self.attention_layers = attention_layers or [-1]  # ä½¿ç”¨æœ€åä¸€å±‚attention

    def compute_confidence(self, model_output: Dict[str, torch.Tensor],
                           inputs: Optional[torch.Tensor] = None,
                           **kwargs) -> ConfidenceOutput:
        """
        è®¡ç®—NLPç½®ä¿¡åº¦

        Args:
            model_output: æ¨¡å‹è¾“å‡ºå­—å…¸ï¼ŒåŒ…å«ï¼š
                - 'logits': [B, seq_len, vocab_size] æˆ– [B, num_classes]
                - 'attentions': List of [B, num_heads, seq_len, seq_len] (å¯é€‰)
                - 'hidden_states': [B, seq_len, hidden_size] (å¯é€‰)
            inputs: [B, seq_len] token ids (å¯é€‰)

        Returns:
            ConfidenceOutputå¯¹è±¡
        """

        components = {}
        metadata = {}

        if self.task_type == 'classification':
            confidence = self._compute_text_classification_confidence(
                model_output, inputs, components, metadata
            )
        elif self.task_type == 'generation':
            confidence = self._compute_generation_confidence(
                model_output, inputs, components, metadata
            )
        else:
            confidence = self._compute_text_classification_confidence(
                model_output, inputs, components, metadata
            )

        return ConfidenceOutput(
            confidence=confidence,
            components=components,
            metadata=metadata,
            domain='nlp'
        )

    def _compute_text_classification_confidence(self, model_output: Dict,
                                                inputs: Optional[torch.Tensor],
                                                components: Dict, metadata: Dict) -> float:
        """è®¡ç®—æ–‡æœ¬åˆ†ç±»ç½®ä¿¡åº¦"""

        logits = model_output.get('logits')
        if logits is None:
            raise ValueError("æ–‡æœ¬åˆ†ç±»éœ€è¦'logits'è¾“å‡º")

        # å¦‚æœæ˜¯åºåˆ—logitsï¼Œå–å¹³å‡æˆ–æœ€åä¸€ä¸ªtoken
        if logits.dim() == 3:  # [B, seq_len, num_classes]
            logits = logits[:, -1, :]  # å–æœ€åä¸€ä¸ªtoken

        # 1. åŸºç¡€åˆ†ç±»ç½®ä¿¡åº¦ (ä¸Visionç±»ä¼¼)
        probs = F.softmax(logits, dim=-1)
        max_prob_score = probs.max(dim=-1)[0].mean().item()
        components['max_probability'] = max_prob_score

        # 2. è¯­è¨€å­¦ç‰¹å¾ä¸€è‡´æ€§
        linguistic_consistency = 0.0
        if 'hidden_states' in model_output:
            hidden_states = model_output['hidden_states']  # [B, seq_len, D]
            linguistic_consistency = self._compute_linguistic_consistency(hidden_states)
        components['linguistic_consistency'] = linguistic_consistency

        # 3. æ³¨æ„åŠ›ä¸€è‡´æ€§
        attention_consistency = 0.0
        if 'attentions' in model_output:
            attentions = model_output['attentions']
            attention_consistency = self._compute_attention_consistency(attentions)
        components['attention_consistency'] = attention_consistency

        # 4. è¯æ±‡å¤šæ ·æ€§åˆ†æ•° (å¦‚æœæœ‰è¾“å…¥token)
        vocabulary_confidence = 0.0
        if inputs is not None:
            vocabulary_confidence = self._compute_vocabulary_confidence(inputs)
        components['vocabulary_confidence'] = vocabulary_confidence

        # 5. ç»¼åˆç½®ä¿¡åº¦
        weights = {
            'max_probability': 0.4,
            'linguistic_consistency': 0.25,
            'attention_consistency': 0.25,
            'vocabulary_confidence': 0.1
        }

        final_confidence = sum(
            components[key] * weights[key] for key in weights.keys()
        )

        metadata.update({
            'task_type': 'text_classification',
            'num_classes': probs.size(-1),
            'sequence_length': hidden_states.size(1) if 'hidden_states' in model_output else 0
        })

        return final_confidence

    def _compute_linguistic_consistency(self, hidden_states: torch.Tensor) -> float:
        """è®¡ç®—è¯­è¨€å­¦ç‰¹å¾ä¸€è‡´æ€§"""
        # [B, seq_len, hidden_size]
        # è®¡ç®—åºåˆ—å†…hidden statesçš„ä½™å¼¦ç›¸ä¼¼åº¦
        normalized_states = F.normalize(hidden_states, dim=-1)
        # è®¡ç®—ç›¸é‚»tokençš„ç›¸ä¼¼åº¦
        similarities = []
        for i in range(hidden_states.size(1) - 1):
            sim = F.cosine_similarity(
                normalized_states[:, i, :],
                normalized_states[:, i + 1, :],
                dim=-1
            )
            similarities.append(sim.mean().item())

        if similarities:
            consistency = np.mean(similarities)
        else:
            consistency = 0.0

        return consistency

    def _compute_attention_consistency(self, attentions: List[torch.Tensor]) -> float:
        """è®¡ç®—æ³¨æ„åŠ›æ¨¡å¼ä¸€è‡´æ€§"""
        if not attentions:
            return 0.0

        # å–æŒ‡å®šå±‚çš„attention
        layer_idx = self.attention_layers[0]
        if layer_idx >= len(attentions):
            layer_idx = -1

        attention = attentions[layer_idx]  # [B, num_heads, seq_len, seq_len]

        # è®¡ç®—ä¸åŒheadä¹‹é—´çš„ä¸€è‡´æ€§
        B, num_heads, seq_len, _ = attention.shape
        head_similarities = []

        for i in range(num_heads):
            for j in range(i + 1, num_heads):
                # è®¡ç®—head iå’Œhead jçš„æ³¨æ„åŠ›æ¨¡å¼ç›¸ä¼¼åº¦
                head_i = attention[:, i, :, :].flatten(1)  # [B, seq_len^2]
                head_j = attention[:, j, :, :].flatten(1)  # [B, seq_len^2]

                sim = F.cosine_similarity(head_i, head_j, dim=1).mean().item()
                head_similarities.append(sim)

        if head_similarities:
            consistency = np.mean(head_similarities)
        else:
            consistency = 0.0

        return consistency

    def _compute_vocabulary_confidence(self, input_ids: torch.Tensor) -> float:
        """è®¡ç®—è¯æ±‡å¤šæ ·æ€§ç½®ä¿¡åº¦"""
        # ç®€å•å®ç°ï¼šè®¡ç®—unique tokençš„æ¯”ä¾‹
        batch_size, seq_len = input_ids.shape
        unique_ratios = []

        for i in range(batch_size):
            tokens = input_ids[i]
            unique_tokens = torch.unique(tokens)
            unique_ratio = len(unique_tokens) / seq_len
            unique_ratios.append(unique_ratio)

        return np.mean(unique_ratios)

    def _compute_generation_confidence(self, model_output: Dict,
                                       inputs: Optional[torch.Tensor],
                                       components: Dict, metadata: Dict) -> float:
        """è®¡ç®—ç”Ÿæˆä»»åŠ¡ç½®ä¿¡åº¦"""
        # ç®€åŒ–å®ç°ï¼Œä¸»è¦åŸºäºlogits
        logits = model_output.get('logits')
        if logits is None:
            return 0.0

        # å¯¹äºç”Ÿæˆä»»åŠ¡ï¼Œè®¡ç®—åºåˆ—çº§åˆ«çš„ç½®ä¿¡åº¦
        probs = F.softmax(logits, dim=-1)  # [B, seq_len, vocab_size]

        # æ¯ä¸ªpositionçš„æœ€å¤§æ¦‚ç‡
        max_probs = probs.max(dim=-1)[0]  # [B, seq_len]

        # åºåˆ—çº§ç½®ä¿¡åº¦ï¼šæ‰€æœ‰positionæ¦‚ç‡çš„å‡ ä½•å¹³å‡
        sequence_confidence = torch.exp(torch.log(max_probs + 1e-8).mean(dim=-1))
        final_confidence = sequence_confidence.mean().item()

        components['sequence_confidence'] = final_confidence
        metadata['task_type'] = 'generation'

        return final_confidence


class SecurityConfidenceCalculator(BaseConfidenceCalculator):
    """
    Securityé¢†åŸŸç½®ä¿¡åº¦è®¡ç®—å™¨

    æ”¯æŒåœºæ™¯ï¼š
    - å¼‚å¸¸æ£€æµ‹: å¼‚å¸¸åˆ†æ•°ç½®ä¿¡åº¦
    - å…¥ä¾µæ£€æµ‹: å¨èƒçº§åˆ«ç½®ä¿¡åº¦
    - æ¶æ„è½¯ä»¶æ£€æµ‹: æ£€æµ‹ç½®ä¿¡åº¦
    """

    def __init__(self, threshold=0.5):
        super().__init__('security')
        self.threshold = threshold

    def compute_confidence(self, model_output: Dict[str, torch.Tensor],
                           inputs: Optional[torch.Tensor] = None,
                           **kwargs) -> ConfidenceOutput:
        """è®¡ç®—Securityç½®ä¿¡åº¦"""

        components = {}
        metadata = {}

        # å¼‚å¸¸æ£€æµ‹ç½®ä¿¡åº¦
        anomaly_scores = model_output.get('anomaly_scores')  # [B]
        if anomaly_scores is not None:
            # å¼‚å¸¸åˆ†æ•°è½¬æ¢ä¸ºç½®ä¿¡åº¦
            confidence_scores = torch.sigmoid(anomaly_scores)
            components['anomaly_confidence'] = confidence_scores.mean().item()

        # å¨èƒçº§åˆ«ç½®ä¿¡åº¦
        threat_logits = model_output.get('threat_logits')  # [B, num_threat_types]
        if threat_logits is not None:
            threat_probs = F.softmax(threat_logits, dim=-1)
            threat_confidence = threat_probs.max(dim=-1)[0].mean().item()
            components['threat_confidence'] = threat_confidence

        # ç»¼åˆç½®ä¿¡åº¦
        final_confidence = np.mean(list(components.values())) if components else 0.0

        metadata.update({
            'domain': 'security',
            'threshold': self.threshold,
            'components_count': len(components)
        })

        return ConfidenceOutput(
            confidence=final_confidence,
            components=components,
            metadata=metadata,
            domain='security'
        )


class MedicalConfidenceCalculator(BaseConfidenceCalculator):
    """
    Medicalé¢†åŸŸç½®ä¿¡åº¦è®¡ç®—å™¨

    æ”¯æŒåœºæ™¯ï¼š
    - åŒ»å­¦å›¾åƒè¯Šæ–­: ç–¾ç—…åˆ†ç±»ç½®ä¿¡åº¦
    - ç—‡çŠ¶åˆ†æ: è¯Šæ–­ç½®ä¿¡åº¦
    - è¯ç‰©æ¨è: æ¨èç½®ä¿¡åº¦
    """

    def __init__(self, uncertainty_method='mc_dropout'):
        super().__init__('medical')
        self.uncertainty_method = uncertainty_method

    def compute_confidence(self, model_output: Dict[str, torch.Tensor],
                           inputs: Optional[torch.Tensor] = None,
                           **kwargs) -> ConfidenceOutput:
        """è®¡ç®—Medicalç½®ä¿¡åº¦"""

        components = {}
        metadata = {}

        # è¯Šæ–­ç½®ä¿¡åº¦
        diagnosis_logits = model_output.get('diagnosis_logits')  # [B, num_diseases]
        if diagnosis_logits is not None:
            diagnosis_probs = F.softmax(diagnosis_logits, dim=-1)
            diagnosis_confidence = diagnosis_probs.max(dim=-1)[0].mean().item()
            components['diagnosis_confidence'] = diagnosis_confidence

        # ä¸ç¡®å®šæ€§ä¼°è®¡
        uncertainty_scores = model_output.get('uncertainty')  # [B]
        if uncertainty_scores is not None:
            # ä¸ç¡®å®šæ€§è¶Šå°ï¼Œç½®ä¿¡åº¦è¶Šé«˜
            uncertainty_confidence = 1.0 - uncertainty_scores.mean().item()
            components['uncertainty_confidence'] = uncertainty_confidence

        # ä¸´åºŠä¸€è‡´æ€§ (å¦‚æœæœ‰å¤šä¸ªé¢„æµ‹)
        if 'ensemble_predictions' in model_output:
            clinical_consistency = self._compute_clinical_consistency(
                model_output['ensemble_predictions']
            )
            components['clinical_consistency'] = clinical_consistency

        # ç»¼åˆç½®ä¿¡åº¦
        final_confidence = np.mean(list(components.values())) if components else 0.0

        metadata.update({
            'domain': 'medical',
            'uncertainty_method': self.uncertainty_method,
            'components_count': len(components)
        })

        return ConfidenceOutput(
            confidence=final_confidence,
            components=components,
            metadata=metadata,
            domain='medical'
        )

    def _compute_clinical_consistency(self, ensemble_predictions: torch.Tensor) -> float:
        """è®¡ç®—ä¸´åºŠä¸€è‡´æ€§"""
        # [num_models, B, num_classes]
        # è®¡ç®—ä¸åŒæ¨¡å‹é¢„æµ‹çš„ä¸€è‡´æ€§
        predicted_classes = ensemble_predictions.argmax(dim=-1)  # [num_models, B]

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ä¸€è‡´æ€§
        consistency_scores = []
        for i in range(predicted_classes.size(1)):  # éå†batch
            sample_predictions = predicted_classes[:, i]  # [num_models]
            # è®¡ç®—ä¼—æ•°å‡ºç°çš„é¢‘ç‡
            unique_preds, counts = torch.unique(sample_predictions, return_counts=True)
            max_count = counts.max().item()
            consistency = max_count / predicted_classes.size(0)
            consistency_scores.append(consistency)

        return np.mean(consistency_scores)


class UniversalConfidenceCalculator:
    """
    é€šç”¨ç½®ä¿¡åº¦è®¡ç®—å™¨ - UniMatch-Clipçš„æ ¸å¿ƒç»„ä»¶

    ç»Ÿä¸€ç®¡ç†å››ä¸ªé¢†åŸŸçš„ç½®ä¿¡åº¦è®¡ç®—ï¼Œæä¾›ç»Ÿä¸€æ¥å£
    """

    def __init__(self):
        # åˆå§‹åŒ–å„é¢†åŸŸè®¡ç®—å™¨
        self.calculators = {
            'vision': VisionConfidenceCalculator(),
            'nlp': NLPConfidenceCalculator(),
            'security': SecurityConfidenceCalculator(),
            'medical': MedicalConfidenceCalculator()
        }

        self.logger = logging.getLogger("UniversalConfidence")

    def compute_confidence(self, domain: str, model_output: Dict[str, torch.Tensor],
                           inputs: Optional[torch.Tensor] = None,
                           **kwargs) -> ConfidenceOutput:
        """
        ç»Ÿä¸€ç½®ä¿¡åº¦è®¡ç®—æ¥å£

        Args:
            domain: é¢†åŸŸåç§° ('vision', 'nlp', 'security', 'medical')
            model_output: æ¨¡å‹è¾“å‡ºå­—å…¸
            inputs: è¾“å…¥å¼ é‡ (å¯é€‰)
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            ConfidenceOutputå¯¹è±¡
        """

        if domain not in self.calculators:
            raise ValueError(f"ä¸æ”¯æŒçš„é¢†åŸŸ: {domain}")

        calculator = self.calculators[domain]

        try:
            result = calculator.compute_confidence(model_output, inputs, **kwargs)
            self.logger.debug(f"Domain {domain} confidence: {result.confidence:.4f}")
            return result

        except Exception as e:
            self.logger.error(f"è®¡ç®—{domain}ç½®ä¿¡åº¦å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ä½ç½®ä¿¡åº¦
            return ConfidenceOutput(
                confidence=0.1,
                components={'error': 0.1},
                metadata={'error': str(e)},
                domain=domain
            )

    def batch_compute_confidence(self, domain: str,
                                 model_outputs: List[Dict[str, torch.Tensor]],
                                 inputs: Optional[List[torch.Tensor]] = None) -> List[ConfidenceOutput]:
        """æ‰¹é‡è®¡ç®—ç½®ä¿¡åº¦"""

        results = []
        inputs_list = inputs or [None] * len(model_outputs)

        for i, (output, input_tensor) in enumerate(zip(model_outputs, inputs_list)):
            try:
                result = self.compute_confidence(domain, output, input_tensor)
                results.append(result)
            except Exception as e:
                self.logger.error(f"æ‰¹é‡è®¡ç®—ç¬¬{i}ä¸ªæ ·æœ¬å¤±è´¥: {e}")
                results.append(ConfidenceOutput(
                    confidence=0.1,
                    components={'error': 0.1},
                    metadata={'error': str(e), 'batch_index': i},
                    domain=domain
                ))

        return results

    def get_supported_domains(self) -> List[str]:
        """è·å–æ”¯æŒçš„é¢†åŸŸåˆ—è¡¨"""
        return list(self.calculators.keys())

    def update_domain_calculator(self, domain: str, calculator: BaseConfidenceCalculator):
        """æ›´æ–°ç‰¹å®šé¢†åŸŸçš„è®¡ç®—å™¨"""
        self.calculators[domain] = calculator
        self.logger.info(f"æ›´æ–°äº†{domain}é¢†åŸŸçš„ç½®ä¿¡åº¦è®¡ç®—å™¨")


# =====================================================
# æµ‹è¯•å’Œä½¿ç”¨ç¤ºä¾‹
# =====================================================

def test_confidence_calculators():
    """æµ‹è¯•å„é¢†åŸŸç½®ä¿¡åº¦è®¡ç®—å™¨"""

    print("ğŸ§ª æµ‹è¯•è·¨åŸŸç½®ä¿¡åº¦è®¡ç®—å™¨...")

    # åˆ›å»ºé€šç”¨è®¡ç®—å™¨
    universal_calc = UniversalConfidenceCalculator()

    # 1. æµ‹è¯•Visioné¢†åŸŸ
    print("\nğŸ“Š æµ‹è¯•Visionç½®ä¿¡åº¦...")
    vision_output = {
        'logits': torch.randn(4, 10),  # 4ä¸ªæ ·æœ¬ï¼Œ10ç±»åˆ†ç±»
        'features': torch.randn(4, 256)  # ç‰¹å¾å‘é‡
    }

    vision_result = universal_calc.compute_confidence('vision', vision_output)
    print(f"Visionç½®ä¿¡åº¦: {vision_result.confidence:.4f}")
    print(f"Visionç»„ä»¶: {vision_result.components}")

    # 2. æµ‹è¯•NLPé¢†åŸŸ
    print("\nğŸ“ æµ‹è¯•NLPç½®ä¿¡åº¦...")
    nlp_output = {
        'logits': torch.randn(2, 5),  # 2ä¸ªæ ·æœ¬ï¼Œ5ç±»åˆ†ç±»
        'hidden_states': torch.randn(2, 20, 768),  # [B, seq_len, hidden_size]
        'attentions': [torch.randn(2, 12, 20, 20)]  # å•å±‚attention
    }

    nlp_inputs = torch.randint(0, 1000, (2, 20))  # token ids
    nlp_result = universal_calc.compute_confidence('nlp', nlp_output, nlp_inputs)
    print(f"NLPç½®ä¿¡åº¦: {nlp_result.confidence:.4f}")
    print(f"NLPç»„ä»¶: {nlp_result.components}")

    # 3. æµ‹è¯•Securityé¢†åŸŸ
    print("\nğŸ”’ æµ‹è¯•Securityç½®ä¿¡åº¦...")
    security_output = {
        'anomaly_scores': torch.randn(3),  # 3ä¸ªæ ·æœ¬çš„å¼‚å¸¸åˆ†æ•°
        'threat_logits': torch.randn(3, 4)  # 4ç§å¨èƒç±»å‹
    }

    security_result = universal_calc.compute_confidence('security', security_output)
    print(f"Securityç½®ä¿¡åº¦: {security_result.confidence:.4f}")
    print(f"Securityç»„ä»¶: {security_result.components}")

    # 4. æµ‹è¯•Medicalé¢†åŸŸ
    print("\nğŸ¥ æµ‹è¯•Medicalç½®ä¿¡åº¦...")
    medical_output = {
        'diagnosis_logits': torch.randn(2, 6),  # 6ç§ç–¾ç—…
        'uncertainty': torch.rand(2) * 0.3,  # ä¸ç¡®å®šæ€§åˆ†æ•° [0, 0.3]
    }

    medical_result = universal_calc.compute_confidence('medical', medical_output)
    print(f"Medicalç½®ä¿¡åº¦: {medical_result.confidence:.4f}")
    print(f"Medicalç»„ä»¶: {medical_result.components}")

    # 5. æµ‹è¯•æ‰¹é‡è®¡ç®—
    print("\nğŸ“¦ æµ‹è¯•æ‰¹é‡è®¡ç®—...")
    batch_outputs = [vision_output, vision_output]  # 2ä¸ªvisionæ ·æœ¬
    batch_results = universal_calc.batch_compute_confidence('vision', batch_outputs)
    print(f"æ‰¹é‡ç»“æœ: {[r.confidence for r in batch_results]}")

    print("\nâœ… ç½®ä¿¡åº¦è®¡ç®—å™¨æµ‹è¯•å®Œæˆ!")

    return True


def demonstrate_confidence_usage():
    """æ¼”ç¤ºå¦‚ä½•åœ¨å®é™…åœºæ™¯ä¸­ä½¿ç”¨ç½®ä¿¡åº¦è®¡ç®—å™¨"""

    print("\nğŸ¯ ç½®ä¿¡åº¦è®¡ç®—å™¨å®é™…ä½¿ç”¨æ¼”ç¤º...")

    calc = UniversalConfidenceCalculator()

    # æ¨¡æ‹Ÿå®é™…ä½¿ç”¨åœºæ™¯
    scenarios = [
        {
            'domain': 'vision',
            'description': 'å›¾åƒåˆ†ç±» - çŒ«ç‹—è¯†åˆ«',
            'output': {
                'logits': torch.tensor([[2.1, -0.5]]),  # é«˜ç½®ä¿¡åº¦é¢„æµ‹çŒ«
                'features': torch.randn(1, 512)
            }
        },
        {
            'domain': 'nlp',
            'description': 'æƒ…æ„Ÿåˆ†æ - ç§¯æ/æ¶ˆæ',
            'output': {
                'logits': torch.tensor([[0.1, 2.8]]),  # é«˜ç½®ä¿¡åº¦ç§¯æ
                'hidden_states': torch.randn(1, 15, 768)
            }
        },
        {
            'domain': 'security',
            'description': 'å¼‚å¸¸æ£€æµ‹ - ç½‘ç»œæµé‡',
            'output': {
                'anomaly_scores': torch.tensor([3.2]),  # é«˜å¼‚å¸¸åˆ†æ•°
            }
        }
    ]

    for scenario in scenarios:
        result = calc.compute_confidence(
            scenario['domain'],
            scenario['output']
        )

        print(f"\nåœºæ™¯: {scenario['description']}")
        print(f"é¢†åŸŸ: {scenario['domain']}")
        print(f"ç½®ä¿¡åº¦: {result.confidence:.4f}")

        # æ ¹æ®ç½®ä¿¡åº¦ç»™å‡ºå»ºè®®
        if result.confidence > 0.8:
            print("âœ… é«˜ç½®ä¿¡åº¦é¢„æµ‹ï¼Œå¯ç›´æ¥ä½¿ç”¨")
        elif result.confidence > 0.5:
            print("âš ï¸  ä¸­ç­‰ç½®ä¿¡åº¦ï¼Œå»ºè®®äººå·¥å®¡æ ¸")
        else:
            print("âŒ ä½ç½®ä¿¡åº¦ï¼Œæ ‡è®°ä¸ºå›°éš¾æ ·æœ¬")

    print("\nğŸ‰ ä½¿ç”¨æ¼”ç¤ºå®Œæˆ!")


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)

    print("ğŸš€ è·¨åŸŸç½®ä¿¡åº¦è®¡ç®—å™¨æµ‹è¯•å¯åŠ¨")
    print("=" * 50)

    # è¿è¡Œæµ‹è¯•
    test_success = test_confidence_calculators()

    if test_success:
        print("\nğŸ¯ è¿è¡Œä½¿ç”¨æ¼”ç¤º...")
        demonstrate_confidence_usage()

        print("\nğŸ‰ ç½®ä¿¡åº¦è®¡ç®—å™¨å®Œå…¨å°±ç»ª!")
        print("âœ… å¯ä»¥è¿›å…¥å›°éš¾æ ·æœ¬é€‰æ‹©å™¨å¼€å‘é˜¶æ®µ")
    else:
        print("\nâŒ éœ€è¦è°ƒè¯•ç½®ä¿¡åº¦è®¡ç®—å™¨")