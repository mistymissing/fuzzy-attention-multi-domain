"""
Vision é€‚é…å™¨ - UniMatch-Clipé¡¹ç›®è§†è§‰é¢†åŸŸå®ç°

åŠŸèƒ½ï¼š
1. æ”¯æŒå¤šç§è§†è§‰ä»»åŠ¡ï¼šåˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²
2. é›†æˆä¸»æµè§†è§‰æ¨¡å‹ï¼šResNetã€ViTã€YOLOã€DETRç­‰
3. è§†è§‰ç‰¹å®šçš„ç½®ä¿¡åº¦è®¡ç®—å’Œå›°éš¾æ ·æœ¬è¯†åˆ«
4. å¤„ç†å„ç§è§†è§‰å›°éš¾æ ·æœ¬ï¼šé®æŒ¡ã€æ¨¡ç³Šã€å…‰ç…§ã€å°ç›®æ ‡ç­‰
5. å›¾åƒé¢„å¤„ç†å’Œæ•°æ®å¢å¼º

æ”¯æŒçš„å›°éš¾æ ·æœ¬ç±»å‹ï¼š
- é®æŒ¡ç›®æ ‡ (Occlusion)
- æ¨¡ç³Šå›¾åƒ (Blur)
- å…‰ç…§å¼‚å¸¸ (Lighting)
- å°ç›®æ ‡ (Small Objects)
- èƒŒæ™¯å¤æ‚ (Complex Background)
- è§†è§’å˜åŒ– (Viewpoint Changes)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torchvision.models import resnet50, vit_b_16
import numpy as np
from typing import Dict, List, Optional, Tuple, Any, Union
import logging
from PIL import Image
import cv2
from dataclasses import dataclass
import time
import math
from typing import Dict, List, Optional, Tuple, Any, Union
# å¯¼å…¥åŸºç±»
from base_adapter import BaseDomainAdapter, DomainType, AdapterInput, AdapterOutput, AdapterMode

class VisionTaskType:
    """è§†è§‰ä»»åŠ¡ç±»å‹"""
    CLASSIFICATION = "classification"
    DETECTION = "detection"
    SEGMENTATION = "segmentation"
    KEYPOINT = "keypoint"


class VisionBackbone:
    """æ”¯æŒçš„è§†è§‰éª¨å¹²ç½‘ç»œ"""
    RESNET50 = "resnet50"
    VIT_B16 = "vit_b_16"
    EFFICIENTNET = "efficientnet"
    CUSTOM = "custom"


@dataclass
class VisionHardSampleMetrics:
    """è§†è§‰å›°éš¾æ ·æœ¬è¯„ä¼°æŒ‡æ ‡"""
    blur_score: float = 0.0  # æ¨¡ç³Šåº¦åˆ†æ•°
    occlusion_score: float = 0.0  # é®æŒ¡åº¦åˆ†æ•°
    lighting_score: float = 0.0  # å…‰ç…§è´¨é‡åˆ†æ•°
    complexity_score: float = 0.0  # èƒŒæ™¯å¤æ‚åº¦åˆ†æ•°
    size_score: float = 0.0  # ç›®æ ‡å¤§å°åˆ†æ•°
    overall_difficulty: float = 0.0  # æ€»ä½“å›°éš¾åº¦


class VisionAdapter(BaseDomainAdapter):
    """
    Visioné¢†åŸŸé€‚é…å™¨
    ä¸“é—¨å¤„ç†å›¾åƒæ•°æ®ï¼Œæ”¯æŒåˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²ç­‰ä»»åŠ¡
    """

    # ------------------------------------------------------------------ #
    # æ„é€ 
    # ------------------------------------------------------------------ #
    def __init__(self,
                 task_type: str = VisionTaskType.CLASSIFICATION,
                 backbone: str = VisionBackbone.RESNET50,
                 pretrained: bool = True,
                 input_size: Tuple[int, int] = (224, 224),
                 num_classes: Optional[int] = None,
                 enable_hard_sample_analysis: bool = True,
                 **kwargs):
        self.task_type = task_type
        self.backbone_name = backbone
        self.pretrained = pretrained
        self.input_size = input_size
        self.enable_hard_sample_analysis = enable_hard_sample_analysis
        self.max_length = None  # Visionä¸éœ€è¦max_length

        # æ ¹æ®éª¨å¹²ç½‘ç»œç¡®å®šç‰¹å¾ç»´åº¦
        if backbone == VisionBackbone.RESNET50:
            backbone_dim = 2048
        elif backbone == VisionBackbone.VIT_B16:
            backbone_dim = 768
        else:
            backbone_dim = kwargs.get('backbone_dim', 2048)

        # åˆå§‹åŒ–åŸºç±»
        super().__init__(
            domain_type=DomainType.VISION,
            input_dim=backbone_dim,
            num_classes=num_classes,
            **kwargs
        )

        # è§†è§‰ç»„ä»¶
        self.backbone = None
        self.vision_preprocessor = None
        self.hard_sample_analyzer = None

        self._init_vision_components()
        self.logger = logging.getLogger("VisionAdapter")
        self.to(self.device)
    # ------------------------------------------------------------------ #
    # å†…éƒ¨åˆå§‹åŒ–
    # ------------------------------------------------------------------ #
    def _init_vision_components(self):
        self._init_backbone()
        self._init_preprocessor()
        if self.enable_hard_sample_analysis:
            self._init_hard_sample_analyzer()

    def _init_backbone(self):
        """
        åˆå§‹åŒ–è§†è§‰backbone
        æ”¯æŒ: resnet50, vit_b_16, custom
        """
        if self.backbone_name == "resnet50":
            from torchvision.models import resnet50
            backbone = resnet50(weights=None)  # è¿™é‡Œå¯ä»¥æ”¹ pretrained=True
            # å»æ‰æœ€åçš„åˆ†ç±»å±‚ï¼Œä¿ç•™ç‰¹å¾æŠ½å–éƒ¨åˆ†
            modules = list(backbone.children())[:-1]
            self.backbone = nn.Sequential(*modules, nn.Flatten())
            self.feature_dim = backbone.fc.in_features  # resnet50 çš„è¾“å‡ºç»´åº¦ 2048

        elif self.backbone_name == "vit_b_16":
            from torchvision.models import vit_b_16
            backbone = vit_b_16(weights=None)
            self.backbone = backbone
            self.feature_dim = backbone.heads.head.in_features  # ViT è¾“å‡ºç»´åº¦

        elif self.backbone_name == "custom":
            # ç®€å•CNN backbone
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),

                nn.Conv2d(64, 128, kernel_size=3, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(inplace=True),

                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Flatten(),
                nn.Linear(128, getattr(self, "input_dim", 256))  # é»˜è®¤ 256 ç»´è¾“å‡º
            )
            self.feature_dim = getattr(self, "input_dim", 256)

        else:
            raise ValueError(f"âŒ Unsupported backbone type: {self.backbone_name}")

    def _init_preprocessor(self):
        # æ¨ç†ç”¨
        self.vision_preprocessor = transforms.Compose([
            transforms.Resize(self.input_size),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])
        # è®­ç»ƒç”¨ï¼ˆå«å¢å¼ºï¼‰
        self.augmentation_transforms = transforms.Compose([
            transforms.Resize((int(self.input_size[0] * 1.1),
                               int(self.input_size[1] * 1.1))),
            transforms.RandomCrop(self.input_size),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ColorJitter(brightness=0.2, contrast=0.2,
                                   saturation=0.2, hue=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])
        self.logger.info(f"Initialized preprocessor for size: {self.input_size}")

    def _init_hard_sample_analyzer(self):
        self.hard_sample_analyzer = VisionHardSampleAnalyzer()
        self.logger.info("Initialized hard sample analyzer")

    # ------------------------------------------------------------------ #
    # é¢„å¤„ç†
    # ------------------------------------------------------------------ #
    def preprocess_data(self, raw_data: Any) -> torch.Tensor:
        if isinstance(raw_data, torch.Tensor):
            return raw_data.to(self.device, dtype=torch.float32)

        if isinstance(raw_data, np.ndarray):
            if raw_data.dtype == np.uint8 and raw_data.ndim == 3:
                image = Image.fromarray(raw_data)
            else:
                raise ValueError("Unsupported numpy array format")
        elif isinstance(raw_data, str):
            image = Image.open(raw_data).convert('RGB')
        elif isinstance(raw_data, Image.Image):
            image = raw_data.convert('RGB')
        elif isinstance(raw_data, list):
            batch_tensors = []
            for item in raw_data:
                t = self.preprocess_data(item)
                batch_tensors.append(t[0] if t.dim() == 4 else t)
            return torch.stack(batch_tensors).to(self.device)
        else:
            raise ValueError(f"Unsupported raw_data type: {type(raw_data)}")

        transform = self.augmentation_transforms if self.mode == AdapterMode.TRAINING \
            else self.vision_preprocessor
        processed = transform(image)
        return processed.unsqueeze(0).to(self.device) if processed.dim() == 3 \
            else processed.to(self.device)

    # ------------------------------------------------------------------ #
    # æŠ½è±¡æ–¹æ³•å®ç°
    # ------------------------------------------------------------------ #
    def extract_features(self, inputs: Union[AdapterInput, Any]) -> torch.Tensor:
        """æå–è§†è§‰ç‰¹å¾ â†’ [B, feat_dim]"""

        # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœä¸æ˜¯AdapterInputï¼ŒåŒ…è£…æˆAdapterInput
        if not isinstance(inputs, AdapterInput):
            # æ³¨æ„ï¼šä¸è¦ä¼ domainå‚æ•°ï¼Œè®©metadataåŒ…å«domainä¿¡æ¯
            inputs = AdapterInput(
                raw_data=inputs,
                metadata={'domain': 'vision'}  # domainæ”¾åœ¨metadataé‡Œ
            )

        # åç»­ä»£ç ä¿æŒä¸å˜
        if isinstance(inputs.raw_data, torch.Tensor):
            image_tensor = inputs.raw_data
        else:
            image_tensor = self.preprocess_data(inputs.raw_data)

        if image_tensor.dim() != 4:
            raise ValueError(f"Expected 4D tensor [B,C,H,W], got {image_tensor.shape}")

        with torch.set_grad_enabled(self.training):
            if self.backbone_name == VisionBackbone.VIT_B16:
                features = self.backbone(image_tensor)
                if features.dim() > 2:
                    features = features.mean(dim=1)
            else:
                features = self.backbone(image_tensor)
                features = features.flatten(1)
        return features

    # ------------------------------------------------------------------ #
    # ç½®ä¿¡åº¦ & å›°éš¾æ ·æœ¬
    # ------------------------------------------------------------------ #
    def _compute_confidence(self, embeddings: torch.Tensor,
                            logits: torch.Tensor) -> torch.Tensor:
        # 1. åŸºç¡€ç½®ä¿¡åº¦
        base_conf = super()._compute_confidence(embeddings, logits)

        # 2. è§†è§‰å¢å¼º
        bonus = torch.zeros_like(base_conf)
        if logits is not None and logits.numel():
            probs = F.softmax(logits, dim=-1)
            sorted_p, _ = torch.sort(probs, dim=-1, descending=True)
            if sorted_p.size(-1) > 1:
                margin = sorted_p[:, 0] - sorted_p[:, 1]
                bonus += margin * 0.1
            entropy = -(probs * torch.log(probs + 1e-8)).sum(-1)
            max_ent = math.log(probs.size(-1))
            bonus += (1 - entropy / max_ent) * 0.1

        # 3. ç‰¹å¾è´¨é‡
        feature_norm = embeddings.norm(dim=-1)
        bonus += torch.tanh(feature_norm / feature_norm.mean()) * 0.05

        final = torch.clamp(base_conf + bonus, 0.0, 1.0)
        return final

    def analyze_hard_samples(self, inputs: AdapterInput,
                             outputs: AdapterOutput) -> VisionHardSampleMetrics:
        if not self.enable_hard_sample_analysis or self.hard_sample_analyzer is None:
            return VisionHardSampleMetrics()
        return self.hard_sample_analyzer.analyze(inputs.raw_data,
                                                 outputs.confidence_scores)

    # ------------------------------------------------------------------ #
    # å‰å‘
    # ------------------------------------------------------------------ #
    def forward(self, inputs: AdapterInput) -> AdapterOutput:
        """
        Vision é€‚é…å™¨å‰å‘ä¼ æ’­
        åœ¨åŸºç±»æµç¨‹ä¸Šé™„åŠ è§†è§‰å›°éš¾æ ·æœ¬åˆ†æ
        """

        # ä¿å­˜åŸå§‹æ–‡æœ¬ç”¨äºåˆ†æ
        if hasattr(inputs, 'raw_data') and isinstance(inputs.raw_data, (str, list)):
            original_texts = inputs.raw_data if isinstance(inputs.raw_data, list) else [inputs.raw_data]
            if inputs.metadata is None:
                inputs.metadata = {}
            inputs.metadata['original_texts'] = original_texts

        # è°ƒç”¨åŸºç±»çš„å‰å‘ä¼ æ’­
        outputs = super().forward(inputs)

        # æ·»åŠ NLPç‰¹å®šåˆ†æ
        if self.enable_hard_sample_analysis:
            hard_sample_metrics = self.analyze_hard_samples(inputs, outputs)
            outputs.metadata['hard_sample_analysis'] = hard_sample_metrics

        # æ·»åŠ NLPç‰¹å®šå…ƒæ•°æ®
        outputs.metadata.update({
            'task_type': self.task_type,
            'backbone': self.backbone_name,
            'max_length': self.max_length,
            'has_hard_analysis': self.enable_hard_sample_analysis
        })

        return outputs


class VisionHardSampleAnalyzer:
    """è§†è§‰å›°éš¾æ ·æœ¬åˆ†æå™¨"""

    def __init__(self):
        self.logger = logging.getLogger("VisionHardAnalyzer")

    def analyze(self, image_tensor: torch.Tensor,
                confidence_scores: torch.Tensor) -> VisionHardSampleMetrics:
        """
        åˆ†æå›¾åƒçš„å›°éš¾ç¨‹åº¦

        Args:
            image_tensor: å›¾åƒå¼ é‡ [B, C, H, W]
            confidence_scores: ç½®ä¿¡åº¦åˆ†æ•° [B]

        Returns:
            å›°éš¾æ ·æœ¬è¯„ä¼°æŒ‡æ ‡
        """

        if image_tensor.dim() != 4 or image_tensor.size(0) == 0:
            return VisionHardSampleMetrics()

        # å–ç¬¬ä¸€ä¸ªæ ·æœ¬è¿›è¡Œåˆ†æ (ç®€åŒ–å®ç°)
        sample_image = image_tensor[0]  # [C, H, W]
        sample_confidence = confidence_scores[0].item()

        # 1. æ¨¡ç³Šåº¦åˆ†æ
        blur_score = self._analyze_blur(sample_image)

        # 2. å…‰ç…§è´¨é‡åˆ†æ
        lighting_score = self._analyze_lighting(sample_image)

        # 3. å¤æ‚åº¦åˆ†æ
        complexity_score = self._analyze_complexity(sample_image)

        # 4. ç»¼åˆå›°éš¾åº¦ (ç»“åˆç½®ä¿¡åº¦)
        overall_difficulty = 1.0 - sample_confidence  # ç½®ä¿¡åº¦ä½ = å›°éš¾åº¦é«˜

        return VisionHardSampleMetrics(
            blur_score=blur_score,
            lighting_score=lighting_score,
            complexity_score=complexity_score,
            overall_difficulty=overall_difficulty
        )

    def _analyze_blur(self, image: torch.Tensor) -> float:
        """åˆ†æå›¾åƒæ¨¡ç³Šåº¦"""
        try:
            # è½¬æ¢ä¸ºç°åº¦å›¾è¿›è¡Œæ‹‰æ™®æ‹‰æ–¯ç®—å­åˆ†æ
            if image.size(0) == 3:  # RGB
                gray = 0.299 * image[0] + 0.587 * image[1] + 0.114 * image[2]
            else:
                gray = image[0]

            # ç®€åŒ–çš„æ¨¡ç³Šåº¦æ£€æµ‹ (åŸºäºæ¢¯åº¦æ–¹å·®)
            grad_x = torch.diff(gray, dim=1)
            grad_y = torch.diff(gray, dim=0)

            blur_score = 1.0 / (1.0 + torch.var(grad_x).item() + torch.var(grad_y).item())
            return float(blur_score)

        except Exception as e:
            self.logger.warning(f"Blur analysis failed: {e}")
            return 0.0

    def _analyze_lighting(self, image: torch.Tensor) -> float:
        """åˆ†æå…‰ç…§è´¨é‡"""
        try:
            # è®¡ç®—å›¾åƒäº®åº¦åˆ†å¸ƒ
            brightness = image.mean(dim=0)  # [H, W]

            # åˆ†æäº®åº¦åˆ†å¸ƒçš„å‡åŒ€æ€§
            brightness_std = torch.std(brightness).item()
            brightness_mean = torch.mean(brightness).item()

            # è¿‡æš—æˆ–è¿‡äº®éƒ½ä¸å¥½
            lighting_quality = 1.0 - abs(brightness_mean - 0.5) * 2
            lighting_quality *= 1.0 / (1.0 + brightness_std)

            return float(lighting_quality)

        except Exception as e:
            self.logger.warning(f"Lighting analysis failed: {e}")
            return 0.5

    def _analyze_complexity(self, image: torch.Tensor) -> float:
        """åˆ†æèƒŒæ™¯å¤æ‚åº¦"""
        try:
            # ç®€åŒ–çš„å¤æ‚åº¦åˆ†æ (åŸºäºè¾¹ç¼˜å¯†åº¦)
            if image.size(0) == 3:
                gray = 0.299 * image[0] + 0.587 * image[1] + 0.114 * image[2]
            else:
                gray = image[0]

            # è®¡ç®—è¾¹ç¼˜å¯†åº¦ï¼ˆSobel æ ¸æ”¾åˆ°åŒä¸€ deviceï¼‰
            device = image.device
            sobel_x = torch.tensor(
                [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32, device=device
            ).unsqueeze(0).unsqueeze(0)
            sobel_y = torch.tensor(
                [[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32, device=device
            ).unsqueeze(0).unsqueeze(0)

            gray_4d = gray.unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]

            edge_x = F.conv2d(gray_4d, sobel_x, padding=1)
            edge_y = F.conv2d(gray_4d, sobel_y, padding=1)

            edge_magnitude = torch.sqrt(edge_x ** 2 + edge_y ** 2)
            complexity = edge_magnitude.mean().item()

            return float(complexity)

        except Exception as e:
            self.logger.warning(f"Complexity analysis failed: {e}")
            return 0.0


# =====================================================
# æµ‹è¯•å’Œä½¿ç”¨ç¤ºä¾‹
# =====================================================

def test_vision_adapter():
    """æµ‹è¯•Visioné€‚é…å™¨åŸºç¡€åŠŸèƒ½"""

    print("ğŸ–¼ï¸ æµ‹è¯•Visioné€‚é…å™¨...")

    # åˆ›å»ºVisioné€‚é…å™¨
    adapter = VisionAdapter(
        task_type=VisionTaskType.CLASSIFICATION,
        backbone=VisionBackbone.RESNET50,
        input_size=(224, 224),
        num_classes=10,
        hidden_dim=512,
        output_dim=128
    )

    print(f"âœ… åˆ›å»ºVisioné€‚é…å™¨: {adapter.get_domain_type().value}")
    print(f"ğŸ“ è¾“å‡ºç»´åº¦: {adapter.get_embedding_dim()}")
    print(f"ğŸ¯ ä»»åŠ¡ç±»å‹: {adapter.task_type}")
    print(f"ğŸ—ï¸ éª¨å¹²ç½‘ç»œ: {adapter.backbone_name}")

    # åˆ›å»ºæµ‹è¯•å›¾åƒæ•°æ®
    batch_size = 3
    test_images = torch.randn(batch_size, 3, 224, 224)  # æ¨¡æ‹Ÿå›¾åƒæ•°æ®
    test_labels = torch.randint(0, 10, (batch_size,))

    print(f"\nğŸ“Š æµ‹è¯•æ•°æ®: {test_images.shape}")

    # åˆ›å»ºé€‚é…å™¨è¾“å…¥
    inputs = AdapterInput(
        raw_data=test_images,
        labels=test_labels,
        metadata={'source': 'test'}
    )

    # è®¾ç½®ä¸ºæ¨ç†æ¨¡å¼å¹¶å¤„ç†
    adapter.set_mode(AdapterMode.INFERENCE)

    start_time = time.time()
    outputs = adapter(inputs)
    processing_time = time.time() - start_time

    print(f"\nğŸ“ˆ å¤„ç†ç»“æœ:")
    print(f"åµŒå…¥å½¢çŠ¶: {outputs.embeddings.shape}")
    print(f"Logitså½¢çŠ¶: {outputs.logits.shape}")
    print(f"ç½®ä¿¡åº¦: {outputs.confidence_scores.detach().numpy()}")
    print(f"å›°éš¾æ ·æœ¬: {outputs.is_hard_sample.numpy()}")
    print(f"å¤„ç†æ—¶é—´: {processing_time:.4f}ç§’")

    # å›°éš¾æ ·æœ¬åˆ†æ
    if 'hard_sample_analysis' in outputs.metadata:
        analysis = outputs.metadata['hard_sample_analysis']
        print(f"\nğŸ” å›°éš¾æ ·æœ¬åˆ†æ:")
        print(f"æ¨¡ç³Šåº¦: {analysis.blur_score:.4f}")
        print(f"å…‰ç…§è´¨é‡: {analysis.lighting_score:.4f}")
        print(f"å¤æ‚åº¦: {analysis.complexity_score:.4f}")
        print(f"æ€»ä½“å›°éš¾åº¦: {analysis.overall_difficulty:.4f}")

    # ç»Ÿè®¡ä¿¡æ¯
    stats = adapter.get_statistics()
    print(f"\nğŸ“Š é€‚é…å™¨ç»Ÿè®¡:")
    for key, value in stats.items():
        print(f"{key}: {value}")

    return True


def test_vision_preprocessing():
    """æµ‹è¯•Visioné¢„å¤„ç†åŠŸèƒ½"""

    print("\nğŸ”§ æµ‹è¯•Visioné¢„å¤„ç†...")

    adapter = VisionAdapter(
        backbone=VisionBackbone.RESNET50,
        input_size=(224, 224)
    )

    # æµ‹è¯•ä¸åŒç±»å‹çš„è¾“å…¥
    test_cases = [
        ("Tensorè¾“å…¥", torch.randn(3, 256, 256)),
        ("Numpyè¾“å…¥", np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)),
        ("æ‰¹é‡è¾“å…¥", [torch.randn(3, 200, 200), torch.randn(3, 300, 300)])
    ]

    for case_name, test_input in test_cases:
        try:
            processed = adapter.preprocess_data(test_input)
            print(f"âœ… {case_name}: {processed.shape}")
        except Exception as e:
            print(f"âŒ {case_name}: {e}")

    return True


def test_vision_with_different_backbones():
    """æµ‹è¯•ä¸åŒéª¨å¹²ç½‘ç»œ"""

    print("\nğŸ—ï¸ æµ‹è¯•ä¸åŒéª¨å¹²ç½‘ç»œ...")

    backbones = [
        (VisionBackbone.RESNET50, "ResNet-50"),
        (VisionBackbone.VIT_B16, "ViT-B/16")
    ]

    test_image = torch.randn(2, 3, 224, 224)

    for backbone_type, backbone_name in backbones:
        try:
            print(f"\næµ‹è¯• {backbone_name}...")

            adapter = VisionAdapter(
                backbone=backbone_type,
                num_classes=5,
                output_dim=128
            )

            inputs = AdapterInput(raw_data=test_image)
            outputs = adapter(inputs)

            print(
                f"âœ… {backbone_name}: åµŒå…¥ {outputs.embeddings.shape}, ç½®ä¿¡åº¦å‡å€¼ {outputs.confidence_scores.mean():.4f}")

        except Exception as e:
            print(f"âŒ {backbone_name}: {e}")

    return True


def demonstrate_vision_workflow():
    """æ¼”ç¤ºVisioné€‚é…å™¨çš„å®Œæ•´å·¥ä½œæµç¨‹"""

    print("\nğŸ”„ æ¼”ç¤ºVisioné€‚é…å™¨å·¥ä½œæµç¨‹...")

    # 1. åˆ›å»ºé€‚é…å™¨
    adapter = VisionAdapter(
        task_type=VisionTaskType.CLASSIFICATION,
        backbone=VisionBackbone.RESNET50,
        num_classes=10,
        enable_hard_sample_analysis=True
    )

    # 2. å‡†å¤‡å¤šæ ·åŒ–çš„æµ‹è¯•æ•°æ®
    easy_images = torch.randn(2, 3, 224, 224) * 0.5 + 0.5  # ç›¸å¯¹æ¸…æ™°çš„å›¾åƒ
    hard_images = torch.randn(2, 3, 224, 224) * 1.5  # å™ªå£°è¾ƒå¤§çš„å›¾åƒ

    all_images = torch.cat([easy_images, hard_images], dim=0)
    labels = torch.tensor([1, 3, 7, 9])

    print(f"ğŸ“Š å‡†å¤‡äº† {len(all_images)} å¼ æµ‹è¯•å›¾åƒ")

    # 3. æ‰¹é‡å¤„ç†
    inputs = AdapterInput(
        raw_data=all_images,
        labels=labels,
        metadata={'batch_name': 'demo_batch'}
    )

    outputs = adapter(inputs)

    # 4. åˆ†æç»“æœ
    print(f"\nğŸ“ˆ å¤„ç†ç»“æœåˆ†æ:")
    confidences = outputs.confidence_scores.detach().numpy()
    hard_samples = outputs.is_hard_sample.numpy()

    for i, (conf, is_hard) in enumerate(zip(confidences, hard_samples)):
        sample_type = "å›°éš¾æ ·æœ¬" if is_hard else "ç®€å•æ ·æœ¬"
        print(f"å›¾åƒ {i + 1}: ç½®ä¿¡åº¦ {conf:.4f} - {sample_type}")

    # 5. å›°éš¾æ ·æœ¬è¯¦ç»†åˆ†æ
    if 'hard_sample_analysis' in outputs.metadata:
        print(f"\nğŸ” å›°éš¾æ ·æœ¬è¯¦ç»†åˆ†æ:")
        analysis = outputs.metadata['hard_sample_analysis']
        print(f"æ¨¡ç³Šåº¦åˆ†æ•°: {analysis.blur_score:.4f}")
        print(f"å…‰ç…§è´¨é‡: {analysis.lighting_score:.4f}")
        print(f"èƒŒæ™¯å¤æ‚åº¦: {analysis.complexity_score:.4f}")
        print(f"æ€»ä½“å›°éš¾åº¦: {analysis.overall_difficulty:.4f}")

    # 6. é€‚é…å™¨æ€§èƒ½ç»Ÿè®¡
    stats = adapter.get_statistics()
    print(f"\nğŸ“Š é€‚é…å™¨æ€§èƒ½ç»Ÿè®¡:")
    print(f"å¤„ç†æ ·æœ¬æ•°: {stats['total_samples']}")
    print(f"å›°éš¾æ ·æœ¬æ•°: {stats['hard_samples']}")
    print(f"å›°éš¾æ ·æœ¬æ¯”ä¾‹: {stats['hard_sample_ratio']:.2%}")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {stats['avg_confidence']:.4f}")
    print(f"å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']:.4f}ç§’")

    return True


if __name__ == "__main__":
    print("ğŸš€ Visioné€‚é…å™¨æµ‹è¯•å¯åŠ¨")
    print("=" * 50)

    # åŸºç¡€åŠŸèƒ½æµ‹è¯•
    basic_success = test_vision_adapter()

    if basic_success:
        # é¢„å¤„ç†æµ‹è¯•
        preprocess_success = test_vision_preprocessing()

        # å¤šéª¨å¹²ç½‘ç»œæµ‹è¯•
        backbone_success = test_vision_with_different_backbones()

        # å®Œæ•´å·¥ä½œæµç¨‹æ¼”ç¤º
        workflow_success = demonstrate_vision_workflow()

        if all([preprocess_success, backbone_success, workflow_success]):
            print("\nğŸ‰ Visioné€‚é…å™¨å®Œå…¨å°±ç»ª!")
            print("âœ… å¤šç§éª¨å¹²ç½‘ç»œæ”¯æŒæ­£å¸¸")
            print("âœ… å›¾åƒé¢„å¤„ç†å’Œå¢å¼ºæ­£å¸¸")
            print("âœ… å›°éš¾æ ·æœ¬åˆ†æåŠŸèƒ½æ­£å¸¸")
            print("âœ… ç½®ä¿¡åº¦è®¡ç®—å‡†ç¡®")
            print("\nğŸš€ å¯ä»¥å¼€å§‹NLPé€‚é…å™¨å¼€å‘!")
        else:
            print("\nâš ï¸ éƒ¨åˆ†åŠŸèƒ½éœ€è¦è°ƒè¯•")
    else:
        print("\nâŒ éœ€è¦è°ƒè¯•åŸºç¡€åŠŸèƒ½")