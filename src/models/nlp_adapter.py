"""
NLP é€‚é…å™¨ - UniMatch-Clipé¡¹ç›®è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå®ç°

åŠŸèƒ½ï¼š
1. æ”¯æŒå¤šç§NLPä»»åŠ¡ï¼šåˆ†ç±»ã€åºåˆ—æ ‡æ³¨ã€ç”Ÿæˆã€é—®ç­”
2. é›†æˆä¸»æµNLPæ¨¡å‹ï¼šBERTã€RoBERTaã€GPTã€T5ç­‰
3. NLPç‰¹å®šçš„ç½®ä¿¡åº¦è®¡ç®—å’Œå›°éš¾æ ·æœ¬è¯†åˆ«
4. å¤„ç†å„ç§NLPå›°éš¾æ ·æœ¬ï¼šè®½åˆºã€æ­§ä¹‰ã€é•¿æ–‡æœ¬ã€ç½•è§è¯æ±‡ç­‰
5. æ–‡æœ¬é¢„å¤„ç†å’Œæ•°æ®å¢å¼º

æ”¯æŒçš„å›°éš¾æ ·æœ¬ç±»å‹ï¼š
- è®½åˆºå’Œåè¯­ (Sarcasm/Irony)
- è¯­ä¹‰æ­§ä¹‰ (Semantic Ambiguity)
- é•¿æ–‡æœ¬ç†è§£ (Long Text)
- ç½•è§è¯æ±‡ (Rare Vocabulary)
- å¤æ‚è¯­æ³• (Complex Grammar)
- ä¸Šä¸‹æ–‡ä¾èµ– (Context Dependency)
"""

import random
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel, AutoConfig
import numpy as np
from typing import Dict, List, Optional, Tuple, Any, Union
import logging
import re
import string
from dataclasses import dataclass
import time
from collections import Counter
import math

# å¯¼å…¥åŸºç±»
from base_adapter import BaseDomainAdapter, DomainType, AdapterInput, AdapterOutput, AdapterMode


# =======================
# å®ç”¨å·¥å…·
# =======================
def set_seed(seed: int = 42):
    """è®¾ç½®éšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def configure_logging(level: str = "INFO"):
    """ç®€å•çš„æ—¥å¿—çº§åˆ«é…ç½®"""
    lvl = getattr(logging, level.upper(), logging.INFO)
    logging.basicConfig(
        level=lvl,
        format="%(asctime)s | %(name)s | %(levelname)s | %(message)s"
    )


class NLPTaskType:
    """NLPä»»åŠ¡ç±»å‹"""
    CLASSIFICATION = "classification"
    SEQUENCE_LABELING = "sequence_labeling"
    GENERATION = "generation"
    QUESTION_ANSWERING = "question_answering"
    SENTIMENT_ANALYSIS = "sentiment_analysis"


class NLPBackbone:
    """æ”¯æŒçš„NLPéª¨å¹²æ¨¡å‹"""
    BERT_BASE = "bert-base-uncased"
    ROBERTA_BASE = "roberta-base"
    DISTILBERT = "distilbert-base-uncased"
    CUSTOM = "custom"


@dataclass
class NLPHardSampleMetrics:
    """NLPå›°éš¾æ ·æœ¬è¯„ä¼°æŒ‡æ ‡"""
    sarcasm_score: float = 0.0  # è®½åˆºç¨‹åº¦åˆ†æ•°
    ambiguity_score: float = 0.0  # æ­§ä¹‰ç¨‹åº¦åˆ†æ•°
    complexity_score: float = 0.0  # è¯­æ³•å¤æ‚åº¦åˆ†æ•°
    rare_vocab_score: float = 0.0  # ç½•è§è¯æ±‡åˆ†æ•°
    length_score: float = 0.0  # é•¿åº¦å›°éš¾åº¦åˆ†æ•°
    sentiment_conflict: float = 0.0  # æƒ…æ„Ÿå†²çªåˆ†æ•°
    overall_difficulty: float = 0.0  # æ€»ä½“å›°éš¾åº¦


class NLPAdapter(BaseDomainAdapter):
    """
    NLPé¢†åŸŸé€‚é…å™¨ï¼šè´Ÿè´£å°†åŸå§‹æ–‡æœ¬ -> åˆ†è¯ç¼–ç  -> ç‰¹å¾ -> åˆ†ç±»/åµŒå…¥ -> ç½®ä¿¡åº¦
    """

    def __init__(self,
                 task_type: str = NLPTaskType.CLASSIFICATION,
                 backbone: str = NLPBackbone.BERT_BASE,
                 max_length: int = 512,
                 num_classes: Optional[int] = None,
                 enable_hard_sample_analysis: bool = True,
                 cache_dir: Optional[str] = None,
                 **kwargs):
        """
        Args:
            task_type: NLPä»»åŠ¡ç±»å‹
            backbone: éª¨å¹²æ¨¡å‹ç±»å‹ï¼ˆ'custom' ä½¿ç”¨æœ¬åœ°ç®€åŒ–æ¨¡å‹ï¼‰
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            num_classes: åˆ†ç±»ç±»åˆ«æ•°
            enable_hard_sample_analysis: æ˜¯å¦å¯ç”¨å›°éš¾æ ·æœ¬åˆ†æ
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
        """
        self.task_type = task_type
        self.backbone_name = backbone
        self.max_length = max_length
        self.enable_hard_sample_analysis = enable_hard_sample_analysis
        self.cache_dir = cache_dir

        # å…ˆæŠŠ NLPAdapter è‡ªå·±çš„ kwargs å–å‡ºæ¥ï¼Œé¿å…é€ä¼ ç»™åŸºç±»
        self.hard_sample_threshold = kwargs.pop("hard_sample_threshold", 0.6)
        backbone_dim_override = kwargs.pop('backbone_dim', None)

        # æ ¹æ®éª¨å¹²æ¨¡å‹ç¡®å®šç‰¹å¾ç»´åº¦
        if "bert" in backbone.lower():
            backbone_dim = 768 if "base" in backbone.lower() else 1024
        elif "roberta" in backbone.lower():
            backbone_dim = 768
        elif "distilbert" in backbone.lower():
            backbone_dim = 768
        else:
            backbone_dim = backbone_dim_override or 768

        # å…ˆå»º loggerï¼Œé¿å…å¤‡ç”¨æ¨¡å‹åˆå§‹åŒ–æ—¶ logger æœªå®šä¹‰
        self.logger = logging.getLogger("NLPAdapter")

        # åˆå§‹åŒ–åŸºç±»ï¼ˆé‡Œè¾¹ä¼šç”¨åˆ° input_dim/num_classes ç­‰ï¼‰
        super().__init__(
            domain_type=DomainType.NLP,
            input_dim=backbone_dim,
            num_classes=num_classes,
            **kwargs  # è¿™é‡Œåªä¿ç•™åŸºç±»è®¤è¯†çš„å‚æ•°
        )

        # ===== å½“ backbone_dim != output_dim æ—¶ï¼ŒåŠ ä¸€å±‚ 768->output_dim çš„æŠ•å½± =====
        self._use_proj = (hasattr(self, "output_dim") and self.input_dim != self.output_dim)
        if self._use_proj:
            self.nlp_proj = nn.Linear(self.input_dim, self.output_dim)
            self.nlp_proj.to(self.device)

        # NLPç‰¹å®šç»„ä»¶
        self.tokenizer = None
        self.backbone = None
        self.text_preprocessor = None
        self.hard_sample_analyzer = None

        # åˆå§‹åŒ–NLPç»„ä»¶
        self._init_nlp_components()

    # --------------------------
    # ç»„ä»¶åˆå§‹åŒ–
    # --------------------------
    def _init_nlp_components(self):
        """åˆå§‹åŒ–NLPç‰¹å®šç»„ä»¶"""
        self._init_tokenizer_and_model()
        self._init_text_preprocessor()
        if self.enable_hard_sample_analysis:
            self._init_hard_sample_analyzer()

    def _init_tokenizer_and_model(self):
        """
        åˆå§‹åŒ– tokenizer å’Œ backbone æ¨¡å‹
        æ”¯æŒ custom (ç®€å•æ¨¡å‹) å’Œ huggingface (bert, roberta ç­‰)
        """
        # ç®€å•è‡ªå®šä¹‰æ¨¡å‹ï¼šä¸ä¾èµ–å¤–éƒ¨æƒé‡ï¼Œè®­ç»ƒ/æ¨ç†éƒ½èƒ½è·‘
        if self.backbone_name in ["custom", NLPBackbone.CUSTOM]:
            self._create_simple_nlp_model()
            return

        # HuggingFace æ¨¡å‹
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.backbone_name,
                cache_dir=self.cache_dir,
                use_fast=True
            )
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = getattr(self.tokenizer, "eos_token", None) or '[PAD]'

            config = AutoConfig.from_pretrained(self.backbone_name, cache_dir=self.cache_dir)
            self.backbone = AutoModel.from_pretrained(
                self.backbone_name,
                config=config,
                cache_dir=self.cache_dir
            )

            # é€‚åº¦å†»ç»“ embedding é™ä½æ˜¾å­˜
            if hasattr(self.backbone, 'embeddings'):
                for p in self.backbone.embeddings.parameters():
                    p.requires_grad = False

            # ğŸ”‘ å…³é”®ï¼šHF æ¨¡å‹ä¹Ÿæ˜¾å¼æ¬åˆ° device
            self.backbone.to(self.device)

            self.logger.info(f"Initialized NLP backbone: {self.backbone_name}")

        except Exception as e:
            self.logger.error(f"Failed to initialize NLP model: {e}")
            self._create_simple_nlp_model()

    def _create_simple_nlp_model(self):
        """åˆ›å»ºç®€å•çš„å¤‡ç”¨NLPæ¨¡å‹ï¼ˆä¸ä¾èµ–å¤–éƒ¨æƒé‡ï¼‰"""

        # ç®€å•åˆ†è¯å™¨
        class SimpleTokenizer:
            def __init__(self):
                self.vocab_size = 30522  # BERT vocab size
                self.pad_token = '[PAD]'
                self.pad_token_id = 0
                self.max_length = 512

            def __call__(self, texts, **kwargs):
                if isinstance(texts, str):
                    texts = [texts]

                tokenized = []
                for text in texts:
                    tokens = text.lower().split()[:self.max_length - 2]
                    token_ids = [hash(token) % self.vocab_size for token in tokens]
                    tokenized.append(token_ids)

                max_len = max(len(seq) for seq in tokenized) if tokenized else 1
                attention_masks = []
                for seq in tokenized:
                    attention_mask = [1] * len(seq) + [0] * (max_len - len(seq))
                    seq.extend([self.pad_token_id] * (max_len - len(seq)))
                    attention_masks.append(attention_mask)

                return {
                    'input_ids': torch.tensor(tokenized, dtype=torch.long),
                    'attention_mask': torch.tensor(attention_masks, dtype=torch.long)
                }

        self.tokenizer = SimpleTokenizer()

        # ç®€å• Transformer ç¼–ç å™¨
        self.backbone = nn.Sequential(
            nn.Embedding(30522, self.input_dim),
            nn.TransformerEncoder(
                nn.TransformerEncoderLayer(
                    d_model=self.input_dim,
                    nhead=8,
                    dim_feedforward=2048,
                    dropout=0.1,
                    batch_first=True
                ),
                num_layers=2
            )
        )

        # ğŸ”‘ å…³é”®ï¼šæŠŠå°æ¨¡å‹æƒé‡æ¬åˆ°åŒä¸€ deviceï¼ˆå¦åˆ™ embedding ä¼šç•™åœ¨ CPUï¼‰
        self.backbone.to(self.device)

        # å…¼å®¹ logger å¯èƒ½è¿˜æ²¡å°±ç»ªçš„æƒ…å†µ
        try:
            self.logger.warning("Using simple backup NLP model")
        except Exception:
            print("Using simple backup NLP model")

    def _init_text_preprocessor(self):
        """åˆå§‹åŒ–æ–‡æœ¬é¢„å¤„ç†å™¨"""
        self.text_preprocessor = NLPTextPreprocessor(
            max_length=self.max_length,
            enable_augmentation=(self.mode == AdapterMode.TRAINING)
        )
        self.logger.info("Initialized text preprocessor")

    def _init_hard_sample_analyzer(self):
        """åˆå§‹åŒ–å›°éš¾æ ·æœ¬åˆ†æå™¨"""
        self.hard_sample_analyzer = NLPHardSampleAnalyzer()
        self.logger.info("Initialized NLP hard sample analyzer")

    # --------------------------
    # æ•°æ®æµ
    # --------------------------
    def preprocess_data(self, raw_data: Any) -> Dict[str, torch.Tensor]:
        """
        NLPæ•°æ®é¢„å¤„ç†

        Args:
            raw_data: AdapterInput æˆ– åŸå§‹æ–‡æœ¬æ•°æ® (string, list[str], dict{text|...})

        Returns:
            é¢„å¤„ç†åçš„å¼ é‡å­—å…¸ {'input_ids', 'attention_mask', ...}ï¼Œå¹¶é™„å¸¦ '_original_texts'
        """
        # å…è®¸ç›´æ¥ä¼  AdapterInput
        if isinstance(raw_data, AdapterInput):
            raw_data = raw_data.raw_data

        # ç»Ÿä¸€æˆæ–‡æœ¬åˆ—è¡¨
        if isinstance(raw_data, str):
            texts = [raw_data]
        elif isinstance(raw_data, list):
            texts = raw_data
        elif isinstance(raw_data, dict) and 'text' in raw_data:
            texts = raw_data['text'] if isinstance(raw_data['text'], list) else [raw_data['text']]
        else:
            raise ValueError(f"Unsupported raw_data type for NLP: {type(raw_data)}")

        # æ–‡æœ¬é¢„å¤„ç†
        if self.text_preprocessor:
            texts = [self.text_preprocessor.preprocess(text) for text in texts]

        # åˆ†è¯
        try:
            encoded = self.tokenizer(
                texts,
                padding=True,
                truncation=True,
                max_length=self.max_length,
                return_tensors='pt'
            )
        except Exception as e:
            self.logger.error(f"Tokenization failed: {e}")
            encoded = self.tokenizer(texts)

        # ç§»åŠ¨åˆ°è®¾å¤‡
        for key in encoded:
            if isinstance(encoded[key], torch.Tensor):
                encoded[key] = encoded[key].to(self.device)

        # å¸¦ä¸ŠåŸå§‹æ–‡æœ¬ï¼Œä¾¿äºå›°éš¾æ ·æœ¬åˆ†æ
        if isinstance(encoded, dict):
            encoded["_original_texts"] = texts

        return encoded

    def extract_features(self, inputs: AdapterInput) -> torch.Tensor:
        """
        æå–NLPç‰¹å¾

        Args:
            inputs: AdapterInputï¼Œinputs.raw_data ä¸º tokenizer ç¼–ç åçš„ dict

        Returns:
            NLPç‰¹å¾å¼ é‡ [B, feature_dim]
        """
        encoded_data = inputs.raw_data
        if not isinstance(encoded_data, dict):
            raise ValueError("Expected encoded dict as input to extract_features")

        with torch.set_grad_enabled(self.training):
            try:
                # HuggingFace æ¨¡å‹ï¼ˆæœ‰ config/model_typeï¼‰
                if hasattr(self.backbone, 'config') and hasattr(self.backbone.config, 'model_type'):
                    outputs = self.backbone(**{k: v for k, v in encoded_data.items()
                                              if isinstance(v, torch.Tensor)})
                    if hasattr(outputs, 'last_hidden_state'):
                        features = outputs.last_hidden_state  # [B, seq_len, hidden_dim]
                    elif hasattr(outputs, 'hidden_states'):
                        features = outputs.hidden_states[-1]
                    else:
                        features = outputs[0]
                else:
                    # ç®€å•æ¨¡å‹ï¼ˆnn.Sequentialï¼‰
                    input_ids = encoded_data['input_ids']
                    features = self.backbone(input_ids)

                # æ± åŒ–åˆ°å›ºå®šé•¿åº¦ç‰¹å¾
                if features.dim() == 3:  # [B, seq_len, hidden_dim]
                    if 'attention_mask' in encoded_data:
                        attention_mask = encoded_data['attention_mask'].unsqueeze(-1)
                        denom = attention_mask.sum(dim=1).clamp(min=1)
                        features = (features * attention_mask).sum(dim=1) / denom
                    else:
                        features = features.mean(dim=1)

            except Exception as e:
                self.logger.error(f"Feature extraction failed: {e}")
                batch_size = encoded_data['input_ids'].size(0)
                features = torch.randn(batch_size, self.input_dim, device=self.device)

        return features

    # --------------------------
    # ç½®ä¿¡åº¦ä¸å›°éš¾æ ·æœ¬
    # --------------------------
    def _compute_confidence(self, embeddings: torch.Tensor,
                            logits: Optional[torch.Tensor]) -> torch.Tensor:
        """
        è®¡ç®—NLPä»»åŠ¡çš„ç½®ä¿¡åº¦
        ç»„åˆï¼š
          1) çˆ¶ç±»åŸºç¡€ç½®ä¿¡åº¦ï¼ˆè‹¥å¯ç”¨ï¼‰
          2) é¢„æµ‹ä¸€è‡´æ€§ä¸ç†µ
          3) ç‰¹å¾èŒƒæ•°è´¨é‡
        """
        batch_size = embeddings.size(0)

        # 1) å°è¯•çˆ¶ç±»å®ç°
        base_confidence = None
        try:
            base_confidence = super()._compute_confidence(embeddings, logits)
        except Exception:
            pass
        if base_confidence is None:
            # é€€åŒ–ï¼šsoftmax æœ€å¤§æ¦‚ç‡ or å¸¸æ•°
            if logits is not None and logits.numel() > 0:
                probs = F.softmax(logits, dim=-1)
                base_confidence = probs.max(dim=-1)[0]
            else:
                base_confidence = torch.full((batch_size,), 0.5, device=self.device)

        nlp_conf_bonus = torch.zeros(batch_size, device=self.device)

        # 2) ä¸€è‡´æ€§/ç†µ
        if logits is not None and logits.numel() > 0:
            probs = F.softmax(logits, dim=-1)
            max_probs = probs.max(dim=-1)[0]
            consistency_bonus = torch.log(max_probs + 1e-8) / -10.0
            nlp_conf_bonus += consistency_bonus * 0.1

            entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=-1)
            max_entropy = math.log(probs.size(-1))
            entropy_conf = 1.0 - (entropy / max_entropy)
            nlp_conf_bonus += entropy_conf * 0.1

        # 3) ç‰¹å¾è¡¨ç¤ºè´¨é‡ï¼ˆèŒƒæ•°ï¼‰
        feature_norm = torch.norm(embeddings, dim=-1)
        feature_quality = torch.sigmoid(feature_norm / (feature_norm.mean() + 1e-6)) * 0.05
        nlp_conf_bonus += feature_quality

        final_conf = torch.clamp(base_confidence + nlp_conf_bonus, 0.0, 1.0)
        return final_conf

    def analyze_hard_samples(self, inputs: AdapterInput,
                             outputs: AdapterOutput) -> NLPHardSampleMetrics:
        """åˆ†æNLPå›°éš¾æ ·æœ¬çš„å…·ä½“ç±»å‹"""
        if not self.enable_hard_sample_analysis or self.hard_sample_analyzer is None:
            return NLPHardSampleMetrics()

        # ä¼˜å…ˆä» metadata å–åŸæ–‡åˆ—è¡¨
        texts = None
        if outputs.metadata and isinstance(outputs.metadata, dict):
            texts = outputs.metadata.get('original_texts')

        if not texts:
            # å°è¯•ä» inputs.raw_data é™„å¸¦çš„ _original_texts æ¢å¤
            if isinstance(inputs.raw_data, dict):
                texts = inputs.raw_data.get("_original_texts", None)

        if not texts:
            texts = ["sample text"]

        return self.hard_sample_analyzer.analyze(texts, outputs.confidence_scores)

    # --------------------------
    # å‰å‘
    # --------------------------
    def forward(self, inputs: AdapterInput) -> AdapterOutput:
        """
        å‰å‘ä¼ æ’­
        æµç¨‹: raw_data â†’ preprocess_data â†’ feature extractor â†’ (å¯é€‰)æŠ•å½± â†’ åˆ†ç±» â†’ ç½®ä¿¡åº¦ â†’ å›°éš¾æ ·æœ¬ â†’ è¾“å‡ºå°è£…
        """
        # === è®¡æ—¶ï¼ˆç”¨äºç»Ÿè®¡ï¼‰ ===
        _t0 = time.time()

        # Step 1: æ•°æ®é¢„å¤„ç†ï¼ˆå…è®¸ä¼  AdapterInputï¼‰
        encoded = self.preprocess_data(inputs)

        # æŠŠ encoded åŒ…è£…ä¸º AdapterInputï¼Œä¾¿äº extract_features è¯»å– .raw_data
        encoded_input = AdapterInput(
            raw_data=encoded,
            labels=getattr(inputs, "labels", None),
            metadata=getattr(inputs, "metadata", None)
        )

        # Step 2: ç‰¹å¾æå–ï¼ˆä¾‹å¦‚ [B, 768]ï¼‰
        features = self.extract_features(encoded_input)

        # Step 2.5: ç»´åº¦å¯¹é½ï¼ˆå¦‚ 768->128ï¼Œåªæœ‰åœ¨ self.input_dim != self.output_dim æ—¶æ‰è¿›è¡Œï¼‰
        if getattr(self, "_use_proj", False):
            features_proj = self.nlp_proj(features)  # [B, output_dim]
        else:
            features_proj = features  # [B, input_dim æˆ– output_dim]

        # Step 3: åˆ†ç±»ï¼ˆä½¿ç”¨æŠ•å½±åçš„ç‰¹å¾ï¼‰
        if self.classifier is not None:
            logits = self.classifier(features_proj)  # [B, num_classes]
        else:
            # æ²¡æœ‰åˆ†ç±»å™¨æ—¶å…œåº•ä¸ºå…¨é›¶ logitsï¼ˆå½¢çŠ¶ä¿æŒä¸€è‡´ä»¥å…¼å®¹åç»­æµç¨‹ï¼‰
            batch_size = features_proj.size(0)
            logits = torch.zeros(batch_size, self.num_classes or 1, device=self.device)

        # Step 4: ç½®ä¿¡åº¦ï¼ˆåŸºäºæŠ•å½±åçš„åµŒå…¥ï¼‰
        confidences = self._compute_confidence(features_proj, logits)  # [B]

        # Step 4.5: å›°éš¾æ ·æœ¬è¯†åˆ«ï¼ˆä½¿ç”¨é¢†åŸŸé»˜è®¤é˜ˆå€¼æˆ– self.hard_sample_thresholdï¼‰
        is_hard, difficulty_scores = self._identify_hard_samples(
            confidences,
            threshold=getattr(self, "hard_sample_threshold", None)
        )

        # ç»„ metadataï¼ˆæŠŠåŸæ–‡æ”¾è¿›å»ï¼Œä¾¿äºéš¾æ ·æœ¬åˆ†æ/è°ƒè¯•ï¼‰
        meta = {
            "backbone": str(self.backbone_name),
            "hard_sample_threshold": float(getattr(self, "hard_sample_threshold", 0.6)),
            "batch_size": int(features_proj.size(0)),
        }
        if isinstance(encoded, dict) and "_original_texts" in encoded:
            meta["original_texts"] = encoded["_original_texts"]

        # Step 5: è¾“å‡ºå°è£…ï¼ˆå­—æ®µåï¼šconfidence_scores / is_hard_sample / difficulty_scoresï¼‰
        output = AdapterOutput(
            logits=logits,
            embeddings=features_proj,          # ç»Ÿä¸€è¿”å› output_dim ç»´çš„åµŒå…¥
            confidence_scores=confidences,
            is_hard_sample=is_hard,
            difficulty_scores=difficulty_scores,
            metadata=meta
        )

        # å†™å…¥å¤„ç†æ—¶é—´å¹¶æ›´æ–°ç»Ÿè®¡
        output.processing_time = time.time() - _t0
        self._update_stats(output)

        # å¯é€‰ï¼šå›°éš¾æ ·æœ¬åˆ†æï¼ˆä¸æ”¹å˜ä¸»æµç¨‹ï¼‰
        if self.enable_hard_sample_analysis and self.hard_sample_analyzer is not None:
            try:
                analysis = self.analyze_hard_samples(encoded_input, output)
                output.metadata = output.metadata or {}
                output.metadata["hard_sample_analysis"] = analysis
            except Exception as e:
                self.logger.warning(f"Hard-sample analysis failed: {e}")

        return output


class NLPTextPreprocessor:
    """NLPæ–‡æœ¬é¢„å¤„ç†å™¨"""

    def __init__(self, max_length: int = 512, enable_augmentation: bool = False):
        self.max_length = max_length
        self.enable_augmentation = enable_augmentation
        self.logger = logging.getLogger("NLPPreprocessor")

    def preprocess(self, text: str) -> str:
        """é¢„å¤„ç†å•ä¸ªæ–‡æœ¬"""

        # 1. åŸºç¡€æ¸…ç†
        text = self._basic_cleanup(text)

        # 2. é•¿åº¦æ§åˆ¶
        text = self._control_length(text)

        # 3. æ•°æ®å¢å¼º (è®­ç»ƒæ—¶)
        if self.enable_augmentation:
            text = self._apply_augmentation(text)

        return text

    def _basic_cleanup(self, text: str) -> str:
        """åŸºç¡€æ–‡æœ¬æ¸…ç†"""

        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)

        # ç§»é™¤æ§åˆ¶å­—ç¬¦
        text = ''.join(char for char in text if ord(char) >= 32 or char in '\n\t')

        # å»é™¤é¦–å°¾ç©ºç™½
        text = text.strip()

        return text

    def _control_length(self, text: str) -> str:
        """æ§åˆ¶æ–‡æœ¬é•¿åº¦"""

        # ç®€å•çš„æˆªæ–­ç­–ç•¥
        words = text.split()
        if len(words) > self.max_length // 4:  # ä¼°ç®—tokenæ•°
            text = ' '.join(words[:self.max_length // 4])

        return text

    def _apply_augmentation(self, text: str) -> str:
        """åº”ç”¨æ–‡æœ¬æ•°æ®å¢å¼º"""

        # ç®€åŒ–çš„æ•°æ®å¢å¼º
        # åœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥æ·»åŠ åŒä¹‰è¯æ›¿æ¢ã€å›è¯‘ç­‰

        if len(text) > 10 and np.random.random() < 0.1:
            # éšæœºåˆ é™¤ä¸€äº›æ ‡ç‚¹ç¬¦å·
            for punct in string.punctuation:
                if np.random.random() < 0.1:
                    text = text.replace(punct, '')

        return text


class NLPHardSampleAnalyzer:
    """NLPå›°éš¾æ ·æœ¬åˆ†æå™¨"""

    def __init__(self):
        self.logger = logging.getLogger("NLPHardAnalyzer")

        # è®½åˆºæŒ‡ç¤ºè¯
        self.sarcasm_indicators = [
            'yeah right', 'sure', 'obviously', 'totally', 'absolutely',
            'oh really', 'how original', 'brilliant', 'fantastic'
        ]

        # å¤æ‚è¯­æ³•æ¨¡å¼
        self.complex_patterns = [
            r'\b\w+ing\b.*\b\w+ed\b',  # æ··åˆæ—¶æ€
            r'\b(?:who|which|that)\b.*\b(?:who|which|that)\b',  # åµŒå¥—ä»å¥
            r'\b(?:not only|neither|either)\b',  # å¤æ‚è¿æ¥è¯
        ]

    def analyze(self, texts: List[str],
                confidence_scores: torch.Tensor) -> NLPHardSampleMetrics:
        """
        åˆ†ææ–‡æœ¬çš„å›°éš¾ç¨‹åº¦

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            confidence_scores: ç½®ä¿¡åº¦åˆ†æ•°

        Returns:
            NLPå›°éš¾æ ·æœ¬è¯„ä¼°æŒ‡æ ‡
        """

        if not texts:
            return NLPHardSampleMetrics()

        # å–ç¬¬ä¸€ä¸ªæ ·æœ¬è¿›è¡Œåˆ†æ (ç®€åŒ–å®ç°)
        sample_text = texts[0]
        sample_confidence = confidence_scores[0].item() if len(confidence_scores) > 0 else 0.5

        # 1. è®½åˆºåˆ†æ
        sarcasm_score = self._analyze_sarcasm(sample_text)

        # 2. æ­§ä¹‰åˆ†æ
        ambiguity_score = self._analyze_ambiguity(sample_text)

        # 3. å¤æ‚åº¦åˆ†æ
        complexity_score = self._analyze_complexity(sample_text)

        # 4. ç½•è§è¯æ±‡åˆ†æ
        rare_vocab_score = self._analyze_rare_vocabulary(sample_text)

        # 5. é•¿åº¦åˆ†æ
        length_score = self._analyze_length_difficulty(sample_text)

        # 6. æƒ…æ„Ÿå†²çªåˆ†æ
        sentiment_conflict = self._analyze_sentiment_conflict(sample_text)

        # 7. æ€»ä½“å›°éš¾åº¦
        overall_difficulty = 1.0 - sample_confidence

        return NLPHardSampleMetrics(
            sarcasm_score=sarcasm_score,
            ambiguity_score=ambiguity_score,
            complexity_score=complexity_score,
            rare_vocab_score=rare_vocab_score,
            length_score=length_score,
            sentiment_conflict=sentiment_conflict,
            overall_difficulty=overall_difficulty
        )

    def _analyze_sarcasm(self, text: str) -> float:
        """åˆ†æè®½åˆºç¨‹åº¦"""
        text_lower = text.lower()
        sarcasm_count = sum(1 for indicator in self.sarcasm_indicators if indicator in text_lower)
        return min(sarcasm_count / 3.0, 1.0)  # å½’ä¸€åŒ–

    def _analyze_ambiguity(self, text: str) -> float:
        """åˆ†æè¯­ä¹‰æ­§ä¹‰"""
        # ç®€åŒ–çš„æ­§ä¹‰æ£€æµ‹ï¼šä»£è¯æ¯”ä¾‹
        words = text.split()
        pronouns = ['it', 'this', 'that', 'they', 'them', 'which', 'what']
        pronoun_count = sum(1 for word in words if word.lower() in pronouns)
        return min(pronoun_count / max(len(words), 1) * 5, 1.0)

    def _analyze_complexity(self, text: str) -> float:
        """åˆ†æè¯­æ³•å¤æ‚åº¦"""
        complexity_score = 0.0

        for pattern in self.complex_patterns:
            matches = len(re.findall(pattern, text, re.IGNORECASE))
            complexity_score += matches * 0.2

        return min(complexity_score, 1.0)

    def _analyze_rare_vocabulary(self, text: str) -> float:
        """åˆ†æç½•è§è¯æ±‡"""
        words = text.split()
        # ç®€åŒ–ï¼šé•¿è¯æ±‡ä½œä¸ºç½•è§è¯æ±‡çš„ä»£ç†
        long_words = [word for word in words if len(word) > 8]
        return min(len(long_words) / max(len(words), 1) * 3, 1.0)

    def _analyze_length_difficulty(self, text: str) -> float:
        """åˆ†æé•¿åº¦å›°éš¾åº¦"""
        char_count = len(text)
        # è¿‡çŸ­æˆ–è¿‡é•¿éƒ½å›°éš¾
        optimal_length = 100
        length_diff = abs(char_count - optimal_length) / optimal_length
        return min(length_diff, 1.0)

    def _analyze_sentiment_conflict(self, text: str) -> float:
        """åˆ†ææƒ…æ„Ÿå†²çª"""
        # ç®€åŒ–ï¼šæ­£è´Ÿæƒ…æ„Ÿè¯æ±‡å…±ç°
        positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful']
        negative_words = ['bad', 'terrible', 'awful', 'horrible', 'worst']

        text_lower = text.lower()
        pos_count = sum(1 for word in positive_words if word in text_lower)
        neg_count = sum(1 for word in negative_words if word in text_lower)

        if pos_count > 0 and neg_count > 0:
            return min((pos_count + neg_count) / 5.0, 1.0)
        return 0.0


# =====================================================
# æµ‹è¯•å’Œä½¿ç”¨ç¤ºä¾‹
# =====================================================

def test_nlp_adapter():
    """æµ‹è¯•NLPé€‚é…å™¨åŸºç¡€åŠŸèƒ½"""

    print("ğŸ“ æµ‹è¯•NLPé€‚é…å™¨...")

    # å›ºå®šéšæœºç§å­ï¼Œä¿è¯å¯å¤ç°
    set_seed(42)

    # åˆ›å»ºNLPé€‚é…å™¨ (ä½¿ç”¨ç®€å•æ¨¡å‹é¿å…ä¸‹è½½)
    adapter = NLPAdapter(
        task_type=NLPTaskType.CLASSIFICATION,
        backbone=NLPBackbone.CUSTOM,  # ä½¿ç”¨ç®€å•æ¨¡å‹
        max_length=128,
        num_classes=3,
        hidden_dim=256,
        output_dim=128,
        hard_sample_threshold=0.6
    )

    print(f"âœ… åˆ›å»ºNLPé€‚é…å™¨: {adapter.get_domain_type().value}")
    print(f"ğŸ“ è¾“å‡ºç»´åº¦: {adapter.get_embedding_dim()}")
    print(f"ğŸ¯ ä»»åŠ¡ç±»å‹: {adapter.task_type}")
    print(f"ğŸ—ï¸ éª¨å¹²æ¨¡å‹: {adapter.backbone_name}")

    # åˆ›å»ºæµ‹è¯•æ–‡æœ¬æ•°æ®
    test_texts = [
        "This is a great product! I love it.",
        "The movie was terrible and boring.",
        "Yeah right, like that's going to work... absolutely brilliant idea!"
    ]
    test_labels = torch.tensor([1, 0, 2])  # æ­£é¢ã€è´Ÿé¢ã€è®½åˆº

    print(f"\nğŸ“Š æµ‹è¯•æ•°æ®: {len(test_texts)} æ¡æ–‡æœ¬")

    # åˆ›å»ºé€‚é…å™¨è¾“å…¥
    inputs = AdapterInput(
        raw_data=test_texts,
        labels=test_labels,
        metadata={'source': 'test'}
    )

    # è®¾ç½®ä¸ºæ¨ç†æ¨¡å¼å¹¶å¤„ç†
    adapter.set_mode(AdapterMode.INFERENCE)

    start_time = time.time()
    outputs = adapter(inputs)
    processing_time = time.time() - start_time

    print(f"\nğŸ“ˆ å¤„ç†ç»“æœ:")
    print(f"åµŒå…¥å½¢çŠ¶: {outputs.embeddings.shape}")
    print(f"Logitså½¢çŠ¶: {outputs.logits.shape}")
    print(f"ç½®ä¿¡åº¦: {outputs.confidence_scores.detach().cpu().numpy()}")
    print(f"å›°éš¾æ ·æœ¬: {outputs.is_hard_sample.detach().cpu().numpy()}")
    print(f"å¤„ç†æ—¶é—´: {processing_time:.4f}ç§’")

    # å›°éš¾æ ·æœ¬åˆ†æ
    if 'hard_sample_analysis' in outputs.metadata:
        analysis = outputs.metadata['hard_sample_analysis']
        print(f"\nğŸ” å›°éš¾æ ·æœ¬åˆ†æ:")
        print(f"è®½åˆºåˆ†æ•°: {analysis.sarcasm_score:.4f}")
        print(f"æ­§ä¹‰åˆ†æ•°: {analysis.ambiguity_score:.4f}")
        print(f"å¤æ‚åº¦: {analysis.complexity_score:.4f}")
        print(f"ç½•è§è¯æ±‡: {analysis.rare_vocab_score:.4f}")
        print(f"é•¿åº¦å›°éš¾: {analysis.length_score:.4f}")
        print(f"æƒ…æ„Ÿå†²çª: {analysis.sentiment_conflict:.4f}")
        print(f"æ€»ä½“å›°éš¾åº¦: {analysis.overall_difficulty:.4f}")

    # ç»Ÿè®¡ä¿¡æ¯
    stats = adapter.get_statistics()
    print(f"\nğŸ“Š é€‚é…å™¨ç»Ÿè®¡:")
    for key, value in stats.items():
        print(f"{key}: {value}")

    return True


def test_nlp_preprocessing():
    """æµ‹è¯•NLPé¢„å¤„ç†åŠŸèƒ½"""

    print("\nğŸ”§ æµ‹è¯•NLPé¢„å¤„ç†...")

    set_seed(42)
    adapter = NLPAdapter(
        backbone=NLPBackbone.CUSTOM,
        max_length=64
    )

    # æµ‹è¯•ä¸åŒç±»å‹çš„æ–‡æœ¬è¾“å…¥
    test_cases = [
        ("ç®€å•æ–‡æœ¬", "Hello world!"),
        ("é•¿æ–‡æœ¬",
         "This is a very long sentence that should be truncated because it exceeds the maximum length limit set for this test case and will be cut off."),
        ("åˆ—è¡¨è¾“å…¥", ["First text", "Second text", "Third text"]),
        ("å­—å…¸è¾“å…¥", {"text": "Dictionary input text"})
    ]

    for case_name, test_input in test_cases:
        try:
            processed = adapter.preprocess_data(test_input)
            input_ids_shape = processed['input_ids'].shape
            print(f"âœ… {case_name}: {input_ids_shape}")
        except Exception as e:
            print(f"âŒ {case_name}: {e}")

    return True


def test_nlp_edge_cases():
    """æ–°å¢ï¼šè¾¹ç•Œç”¨ä¾‹æµ‹è¯•ï¼ˆç©ºæ–‡æœ¬ & è¶…é•¿æ–‡æœ¬ï¼‰"""
    print("\nğŸ§ª æµ‹è¯•NLPè¾¹ç•Œç”¨ä¾‹...")

    set_seed(42)
    adapter = NLPAdapter(
        backbone=NLPBackbone.CUSTOM,
        max_length=32,
        num_classes=2,
        hidden_dim=128,
        output_dim=64,
        hard_sample_threshold=0.55
    )
    adapter.set_mode(AdapterMode.INFERENCE)

    # 1) ç©ºæ–‡æœ¬ / çº¯ç©ºç™½
    empty_cases = ["", "   \t\n  "]
    try:
        outputs = adapter(AdapterInput(raw_data=empty_cases))
        print("âœ… ç©ºæ–‡æœ¬ï¼šå¯ä»¥æ­£å¸¸å¤„ç†")
        print("ç½®ä¿¡åº¦:", outputs.confidence_scores.detach().cpu().numpy())
        print("å›°éš¾æ ·æœ¬:", outputs.is_hard_sample.detach().cpu().numpy())
    except Exception as e:
        print("âŒ ç©ºæ–‡æœ¬å¤„ç†å¼‚å¸¸ï¼š", e)

    # 2) è¶…é•¿æ–‡æœ¬ï¼ˆé‡å¤æ‹¼æ¥åˆ¶é€ é•¿æ–‡æœ¬ï¼‰
    long_text = "Lorem ipsum dolor sit amet, consectetur adipiscing elit. " * 100
    try:
        outputs = adapter(AdapterInput(raw_data=[long_text]))
        print("âœ… è¶…é•¿æ–‡æœ¬ï¼šå¯ä»¥æ­£å¸¸å¤„ç†")
        print("input_len:", len(adapter.preprocess_data([long_text])["_original_texts"][0].split()))
        print("ç½®ä¿¡åº¦:", outputs.confidence_scores.detach().cpu().numpy())
    except Exception as e:
        print("âŒ è¶…é•¿æ–‡æœ¬å¤„ç†å¼‚å¸¸ï¼š", e)

    return True


def demonstrate_nlp_workflow():
    """æ¼”ç¤ºNLPé€‚é…å™¨çš„å®Œæ•´å·¥ä½œæµç¨‹"""

    print("\nğŸ”„ æ¼”ç¤ºNLPé€‚é…å™¨å·¥ä½œæµç¨‹...")

    set_seed(42)
    # 1. åˆ›å»ºé€‚é…å™¨
    adapter = NLPAdapter(
        task_type=NLPTaskType.SENTIMENT_ANALYSIS,
        backbone=NLPBackbone.CUSTOM,
        num_classes=3,  # æ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§
        enable_hard_sample_analysis=True
    )

    # 2. å‡†å¤‡å¤šæ ·åŒ–çš„æµ‹è¯•æ–‡æœ¬
    test_samples = [
        "I absolutely love this product! Best purchase ever!",  # ç®€å•æ­£é¢
        "This item is okay, nothing special.",  # ä¸­æ€§
        "Worst product ever, complete waste of money!",  # ç®€å•è´Ÿé¢
        "Yeah right, this is TOTALLY the best thing ever... NOT!",  # è®½åˆº/å›°éš¾
        "The intricate complexities of this multifaceted contraption confound comprehension."  # å¤æ‚è¯­è¨€
    ]

    labels = torch.tensor([2, 1, 0, 0, 1])  # 2=æ­£é¢, 1=ä¸­æ€§, 0=è´Ÿé¢

    print(f"ğŸ“Š å‡†å¤‡äº† {len(test_samples)} æ¡æµ‹è¯•æ–‡æœ¬")

    # 3. æ‰¹é‡å¤„ç†
    inputs = AdapterInput(
        raw_data=test_samples,
        labels=labels,
        metadata={'batch_name': 'sentiment_demo'}
    )

    outputs = adapter(inputs)

    # 4. åˆ†æç»“æœ
    print(f"\nğŸ“ˆ å¤„ç†ç»“æœåˆ†æ:")
    confidences = outputs.confidence_scores.detach().cpu().numpy()
    hard_samples = outputs.is_hard_sample.detach().cpu().numpy()

    for i, (text, conf, is_hard) in enumerate(zip(test_samples, confidences, hard_samples)):
        sample_type = "å›°éš¾æ ·æœ¬" if is_hard else "ç®€å•æ ·æœ¬"
        preview = text[:50] + ('...' if len(text) > 50 else '')
        print(f"\næ–‡æœ¬ {i + 1}: {sample_type}")
        print(f"  å†…å®¹: {preview}")
        print(f"  ç½®ä¿¡åº¦: {conf:.4f}")

    # 5. å›°éš¾æ ·æœ¬è¯¦ç»†åˆ†æ
    if 'hard_sample_analysis' in outputs.metadata:
        print(f"\nğŸ” å›°éš¾æ ·æœ¬è¯¦ç»†åˆ†æ:")
        analysis = outputs.metadata['hard_sample_analysis']
        print(f"è®½åˆºç¨‹åº¦: {analysis.sarcasm_score:.4f}")
        print(f"è¯­ä¹‰æ­§ä¹‰: {analysis.ambiguity_score:.4f}")
        print(f"è¯­æ³•å¤æ‚åº¦: {analysis.complexity_score:.4f}")
        print(f"ç½•è§è¯æ±‡: {analysis.rare_vocab_score:.4f}")
        print(f"é•¿åº¦å›°éš¾: {analysis.length_score:.4f}")
        print(f"æƒ…æ„Ÿå†²çª: {analysis.sentiment_conflict:.4f}")

    # 6. æ€§èƒ½ç»Ÿè®¡
    stats = adapter.get_statistics()
    print(f"\nğŸ“Š é€‚é…å™¨æ€§èƒ½ç»Ÿè®¡:")
    print(f"å¤„ç†æ–‡æœ¬æ•°: {stats['total_samples']}")
    print(f"å›°éš¾æ ·æœ¬æ•°: {stats['hard_samples']}")
    print(f"å›°éš¾æ ·æœ¬æ¯”ä¾‹: {stats['hard_sample_ratio']:.2%}")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {stats['avg_confidence']:.4f}")
    print(f"å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']:.4f}ç§’")

    return True


if __name__ == "__main__":
    # å»ºè®®ï¼šåœ¨ä¸»å…¥å£è®¾ç½®æ—¥å¿—çº§åˆ«ä¸éšæœºç§å­
    configure_logging("INFO")
    set_seed(42)

    print("ğŸš€ NLPé€‚é…å™¨æµ‹è¯•å¯åŠ¨")
    print("=" * 50)

    # åŸºç¡€åŠŸèƒ½æµ‹è¯•
    basic_success = test_nlp_adapter()

    if basic_success:
        # é¢„å¤„ç†æµ‹è¯•
        preprocess_success = test_nlp_preprocessing()

        # è¾¹ç•Œç”¨ä¾‹æµ‹è¯•
        edge_success = test_nlp_edge_cases()

        # å®Œæ•´å·¥ä½œæµç¨‹æ¼”ç¤º
        workflow_success = demonstrate_nlp_workflow()

        if all([preprocess_success, workflow_success, edge_success]):
            print("\nğŸ‰ NLPé€‚é…å™¨å®Œå…¨å°±ç»ª!")
            print("âœ… æ–‡æœ¬é¢„å¤„ç†å’Œåˆ†è¯æ­£å¸¸")
            print("âœ… å›°éš¾æ ·æœ¬åˆ†æåŠŸèƒ½æ­£å¸¸")
            print("âœ… ç½®ä¿¡åº¦è®¡ç®—å‡†ç¡®")
            print("âœ… è®½åˆºã€æ­§ä¹‰ã€å¤æ‚è¯­æ³•æ£€æµ‹æ­£å¸¸")
            print("\nğŸŒŸ Day 1 æ™šä¸Šä»»åŠ¡å…¨éƒ¨å®Œæˆ!")
            print("ğŸŠ UniMatch-Clip æ ¸å¿ƒç»„ä»¶å¼€å‘é˜¶æ®µåœ†æ»¡ç»“æŸ!")
        else:
            print("\nâš ï¸ éƒ¨åˆ†åŠŸèƒ½éœ€è¦è°ƒè¯•")
    else:
        print("\nâŒ éœ€è¦è°ƒè¯•åŸºç¡€åŠŸèƒ½")
